{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/training_datasets/\n",
    "download dataset18 and dataset27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import matplotlib.pyplot as plt\n",
    "# from __future__ import print_function\n",
    "# import numpy as np\n",
    "# import math\n",
    "import os\n",
    "# import numpy\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for loading data from files\n",
    "\n",
    "segments_database - segments of data, labels_database - labels of classes (\"other\", \"car\", \"building\"), positions_database -\n",
    "places of segments\n",
    "\n",
    "merge_events_database - ?\n",
    "matches_database - ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segments(folder=folder, filename=\"segments_database.csv\"):\n",
    "    # container\n",
    "    segments = []\n",
    "\n",
    "    # extract and store point data\n",
    "    from pandas import read_csv\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    extracted_data = read_csv(file_path, delimiter=\" \").values\n",
    "\n",
    "    segment_ids = extracted_data[:, 0].astype(int)\n",
    "    duplicate_ids = extracted_data[:, 1].astype(int)\n",
    "    points = extracted_data[:, 2:]\n",
    "\n",
    "#     complete_ids = list(zip(segment_ids, duplicate_ids))\n",
    "    id_changes = []\n",
    "    for i in range(len(segment_ids)):\n",
    "        if i > 0 and (segment_ids[i] != (segment_ids[i - 1]) or duplicate_ids[i]!=duplicate_ids[i-1]):\n",
    "            id_changes.append(i)\n",
    "#     complete_ids = list(zip(segment_ids, duplicate_ids))\n",
    "#     id_changes = []\n",
    "#     for i, complete_id in enumerate(complete_ids):\n",
    "#         if i > 0 and complete_id != complete_ids[i - 1]:\n",
    "#             id_changes.append(i)\n",
    "            \n",
    "    segments = np.split(points, id_changes)\n",
    "\n",
    "    segment_ids = [ids[0] for ids in np.split(segment_ids, id_changes)]\n",
    "    duplicate_ids = [ids[0] for ids in np.split(duplicate_ids, id_changes)]\n",
    "\n",
    "    if len(set(zip(segment_ids, duplicate_ids))) != len(segment_ids):\n",
    "        raise ValueError(\n",
    "            \"Id collision when importing segments. Two segments with same id exist in file.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"  Found \"\n",
    "        + str(len(segments))\n",
    "        + \" segments from \"\n",
    "        + str(np.unique(segment_ids).size)\n",
    "        + \" sequences\"\n",
    "    )\n",
    "    return segments, segment_ids, duplicate_ids\n",
    "\n",
    "def load_segments_no_duplicates(folder=folder, filename=\"segments_database.csv\"):\n",
    "    # container\n",
    "    segments = []\n",
    "\n",
    "    # extract and store point data\n",
    "    from pandas import read_csv\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    extracted_data = read_csv(file_path, delimiter=\" \").values\n",
    "\n",
    "    segment_ids = extracted_data[:, 0].astype(int)\n",
    "    points = extracted_data[:, 1:]\n",
    "\n",
    "    id_changes = []\n",
    "    for i, segment_id in enumerate(segment_ids):\n",
    "        if i > 0 and segment_id != segment_ids[i - 1]:\n",
    "            id_changes.append(i)\n",
    "\n",
    "    segments = np.split(points, id_changes)\n",
    "\n",
    "    segment_ids = [ids[0] for ids in np.split(segment_ids, id_changes)]\n",
    "\n",
    "    print(\"  Found \" + str(len(segments)) + \" segments.\")\n",
    "    return segments, segment_ids\n",
    "\n",
    "\n",
    "def load_positions(folder=folder, filename=\"positions_database.csv\"):\n",
    "    segment_ids = []\n",
    "    duplicate_ids = []\n",
    "    positions = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                segment_ids.append(int(split_line[0]))\n",
    "                duplicate_ids.append(int(split_line[1]))\n",
    "\n",
    "                segment_position = list(map(float, split_line[2:]))\n",
    "                positions.append(segment_position)\n",
    "\n",
    "    print(\"  Found positions for \" + str(len(positions)) + \" segments\")\n",
    "    return positions, segment_ids, duplicate_ids\n",
    "\n",
    "\n",
    "def load_labels(folder=folder, filename=\"labels_database.csv\"):\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                segment_ids.append(int(split_line[0]))\n",
    "                labels.append(int(split_line[1]))\n",
    "\n",
    "    print(\"  Found labels for \" + str(len(labels)) + \" segment ids\")\n",
    "    return labels, segment_ids\n",
    "\n",
    "\n",
    "def load_features(folder=folder, filename=\"features_database.csv\"):\n",
    "    # containers\n",
    "    segment_ids = []\n",
    "    duplicate_ids = []\n",
    "    features = []\n",
    "    feature_names = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if filename:\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                # feature names\n",
    "                if len(feature_names) == 0:\n",
    "                    feature_names = split_line[2::2]\n",
    "\n",
    "                # id\n",
    "                segment_id = split_line[0]\n",
    "                segment_ids.append(int(segment_id))\n",
    "                duplicate_id = split_line[1]\n",
    "                duplicate_ids.append(int(duplicate_id))\n",
    "\n",
    "                # feature values\n",
    "                features.append(np.array([float(i) for i in split_line[3::2]]))\n",
    "\n",
    "    print(\"  Found features for \" + str(len(features)) + \" segments\", end=\"\")\n",
    "    if \"autoencoder_feature1\" in feature_names:\n",
    "        print(\"(incl. autoencoder features)\", end=\"\")\n",
    "    print(\" \")\n",
    "    return features, feature_names, segment_ids, duplicate_ids\n",
    "\n",
    "def load_matches(folder=folder, filename=\"matches_database.csv\"):\n",
    "    # containers\n",
    "    matches = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "                matches.append([int(float(i)) for i in split_line if i != \"\"])\n",
    "\n",
    "    print(\"  Found \" + str(len(matches)) + \" matches\")\n",
    "    return np.array(matches)\n",
    "\n",
    "\n",
    "def load_merges(folder=folder, filename=\"merge_events_database.csv\"):\n",
    "    merge_timestamps = []\n",
    "    merges = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path) as inputfile:\n",
    "        for line in inputfile:\n",
    "            split_line = line.strip().split(\" \")\n",
    "            merge_timestamps.append(int(split_line[0]))\n",
    "            merges.append(list(map(int, split_line[1:])))\n",
    "\n",
    "    print(\"  Found \" + str(len(merges)) + \" merge events\")\n",
    "    return merges, merge_timestamps\n",
    "\n",
    "def load_list_of_lists(path):\n",
    "    with open(path) as infile:\n",
    "        list_of_lists = [line.strip().split(\" \") for line in infile]\n",
    "    return convert_strings_to_floats_in_list_of_lists(list_of_lists)\n",
    "\n",
    "\n",
    "def convert_strings_to_floats_in_list_of_lists(list_of_lists):\n",
    "    result = []\n",
    "    for list_ in list_of_lists:\n",
    "        result_line = []\n",
    "        for item in list_:\n",
    "            try:\n",
    "                num = float(item)\n",
    "                if num.is_integer():\n",
    "                    num = int(num)\n",
    "                result_line.append(num)\n",
    "            except:\n",
    "                result_line.append(item)\n",
    "        result.append(result_line)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Rotation, voxelization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        augment_angle=0.0,\n",
    "        augment_remove_random_min=0.0,\n",
    "        augment_remove_random_max=0.0,\n",
    "        augment_remove_plane_min=0.0,\n",
    "        augment_remove_plane_max=0.0,\n",
    "        augment_jitter=0.0,\n",
    "        align=\"none\",\n",
    "        voxelize=True,\n",
    "        scale_method=\"fixed\",\n",
    "        center_method=\"none\",\n",
    "        scale=(1, 1, 1),\n",
    "        voxels=(1, 1, 1),\n",
    "        remove_mean=False,\n",
    "        remove_std=False,\n",
    "        batch_size=16,\n",
    "        scaler_train_passes=1,\n",
    "    ):\n",
    "        self.augment_remove_random_min = augment_remove_random_min\n",
    "        self.augment_remove_random_max = augment_remove_random_max\n",
    "        self.augment_remove_plane_min = augment_remove_plane_min\n",
    "        self.augment_remove_plane_max = augment_remove_plane_max\n",
    "        self.augment_angle = augment_angle\n",
    "        self.augment_jitter = augment_jitter\n",
    "\n",
    "        self.align = align\n",
    "        self.voxelize = voxelize\n",
    "        self.scale_method = scale_method\n",
    "        self.center_method = center_method\n",
    "        self.scale = np.array(scale)\n",
    "        self.voxels = np.array(voxels)\n",
    "        self.remove_mean = remove_mean\n",
    "        self.remove_std = remove_std\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler_train_passes = scaler_train_passes\n",
    "        self._scaler_exists = False\n",
    "\n",
    "        min_voxel_side_length_m = 0.1\n",
    "        self.min_scale = self.voxels * min_voxel_side_length_m\n",
    "\n",
    "        self.last_scales = []\n",
    "\n",
    "    def init_segments(\n",
    "        self, segments, classes, positions=None, train_ids=None, scaler_path=None\n",
    "    ):\n",
    "\n",
    "        self.segments = segments\n",
    "        self.classes = np.array(classes)\n",
    "\n",
    "        if self.align == \"robot\":\n",
    "            assert positions is not None\n",
    "            self.segments = self._align_robot(self.segments, positions)\n",
    "\n",
    "        # check if we need to train a scaler\n",
    "        if self.remove_mean or self.remove_std:\n",
    "            if scaler_path is None:\n",
    "                assert train_ids is not None\n",
    "                self._train_scaler(train_ids)\n",
    "            else:\n",
    "                self.load_scaler(scaler_path)\n",
    "\n",
    "    def get_processed(self, segment_ids, train=True, normalize=True):\n",
    "        batch_segments = []\n",
    "        for i in segment_ids:\n",
    "            batch_segments.append(self.segments[i])\n",
    "\n",
    "        batch_segments = self.process(batch_segments, train, normalize)\n",
    "        batch_classes = self.classes[segment_ids]\n",
    "\n",
    "        return batch_segments, batch_classes\n",
    "\n",
    "    def process(self, segments, train=True, normalize=True):\n",
    "        # augment through distorsions\n",
    "        if train and self.augment_remove_random_max > 0:\n",
    "            segments = self._augment_remove_random(segments)\n",
    "\n",
    "        if train and self.augment_remove_plane_max > 0:\n",
    "            segments = self._augment_remove_plane(segments)\n",
    "\n",
    "        # align after distorsions\n",
    "        if self.align == \"eigen\":\n",
    "            segments = self._align_eigen(segments)\n",
    "\n",
    "        # augment rotation\n",
    "        if train and self.augment_angle > 0:\n",
    "            segments = self._augment_rotation(segments)\n",
    "\n",
    "        if self.voxelize:\n",
    "            # rescale coordinates and center\n",
    "            segments = self._rescale_coordinates(segments)\n",
    "\n",
    "            # randomly displace the segment\n",
    "            if train and self.augment_jitter > 0:\n",
    "                segments = self._augment_jitter(segments)\n",
    "\n",
    "            # insert into voxel grid\n",
    "            segments = self._voxelize(segments)\n",
    "\n",
    "            # remove mean and/or std\n",
    "            if normalize and self._scaler_exists:\n",
    "                segments = self._normalize_voxel_matrix(segments)\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def get_n_batches(self, train=True):\n",
    "        if train:\n",
    "            return self.n_batches_train\n",
    "        else:\n",
    "            return self.n_batches_test\n",
    "\n",
    "    # create rotation matrix that rotates point around\n",
    "    # the origin by an angle theta, expressed in radians\n",
    "    def _get_rotation_matrix_z(self, theta):\n",
    "        R_z = [\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "\n",
    "        return np.array(R_z)\n",
    "\n",
    "    # align according to the robot's position\n",
    "    def _align_robot(self, segments, positions):\n",
    "        aligned_segments = []\n",
    "        for i, seg in enumerate(segments):\n",
    "            center = np.mean(seg, axis=0)\n",
    "\n",
    "            robot_pos = positions[i] - center\n",
    "            seg = seg - center\n",
    "\n",
    "            # angle between robot and x-axis\n",
    "            angle = np.arctan2(robot_pos[1], robot_pos[0])\n",
    "\n",
    "            # align the segment so the robots perspective is along the x-axis\n",
    "            inv_rotation_matrix_z = self._get_rotation_matrix_z(angle)\n",
    "            aligned_seg = np.dot(seg, inv_rotation_matrix_z)\n",
    "\n",
    "            aligned_segments.append(aligned_seg)\n",
    "\n",
    "        return aligned_segments\n",
    "\n",
    "    def _align_eigen(self, segments):\n",
    "        aligned_segments = []\n",
    "        for segment in segments:\n",
    "            # Calculate covariance\n",
    "            center = np.mean(segment, axis=0)\n",
    "\n",
    "            covariance_2d = np.cov(segment[:, :2] - center[:2], rowvar=False, bias=True)\n",
    "\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(covariance_2d)\n",
    "            alignment_rad = np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0])\n",
    "\n",
    "            if eigenvalues[0] < eigenvalues[1]:\n",
    "                alignment_rad = alignment_rad + np.pi / 2\n",
    "\n",
    "            inv_rotation_matrix_z = self._get_rotation_matrix_z(alignment_rad)\n",
    "            aligned_segment = np.dot(segment, inv_rotation_matrix_z)\n",
    "\n",
    "            y_center = np.mean(segment[:, 1])\n",
    "            n_below = np.sum(segment[:, 1] < y_center)\n",
    "\n",
    "            if n_below < segment.shape[0] / 2:\n",
    "                alignment_rad = alignment_rad + np.pi\n",
    "                inv_rotation_matrix_z = self._get_rotation_matrix_z(np.pi)\n",
    "                aligned_segment = np.dot(aligned_segment, inv_rotation_matrix_z)\n",
    "\n",
    "            aligned_segments.append(aligned_segment)\n",
    "\n",
    "        return aligned_segments\n",
    "\n",
    "    # augment with multiple rotation of the same segment\n",
    "    def _augment_rotation(self, segments):\n",
    "        angle_rad = self.augment_angle * np.pi / 180\n",
    "\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            rotation = np.random.uniform(-angle_rad, angle_rad)\n",
    "            segment = np.dot(segment, self._get_rotation_matrix_z(rotation))\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_remove_random(self, segments):\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            # percentage of points to remove\n",
    "            remove = (\n",
    "                np.random.random()\n",
    "                * (self.augment_remove_random_max - self.augment_remove_random_min)\n",
    "                + self.augment_remove_random_min\n",
    "            )\n",
    "\n",
    "            # randomly choose the points\n",
    "            idx = np.arange(segment.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            idx = idx[int(idx.size * remove) :]\n",
    "\n",
    "            segment = segment[idx]\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_remove_plane(self, segments):\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            # center segment\n",
    "            center = np.mean(segment, axis=0)\n",
    "            segment = segment - center\n",
    "\n",
    "            # slice off a section of the segment\n",
    "            while True:\n",
    "                # generate random plane\n",
    "                plane_norm = np.random.random(3) - 0.5\n",
    "                plane_norm = plane_norm / np.sqrt(np.sum(plane_norm ** 2))\n",
    "\n",
    "                # on which side of the plane each point is\n",
    "                sign = np.dot(segment, plane_norm)\n",
    "\n",
    "                # find an offset that removes a desired amount of points\n",
    "                found = False\n",
    "                plane_offsets = np.linspace(\n",
    "                    -np.max(self.scale), np.max(self.scale), 100\n",
    "                )\n",
    "                np.random.shuffle(plane_offsets)\n",
    "                for plane_offset in plane_offsets:\n",
    "                    keep = sign + plane_offset > 0\n",
    "                    remove_percentage = 1 - (np.sum(keep) / float(keep.size))\n",
    "\n",
    "                    if (\n",
    "                        remove_percentage > self.augment_remove_plane_min\n",
    "                        and remove_percentage < self.augment_remove_plane_max\n",
    "                    ):\n",
    "                        segment = segment[keep]\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "            segment = segment + center\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_jitter(self, segments):\n",
    "        jitter_segments = []\n",
    "        for segment in segments:\n",
    "            jitter = np.random.random(3) * 2 - 1\n",
    "            jitter = jitter * self.augment_jitter * self.voxels / 2\n",
    "\n",
    "            segment = segment + jitter\n",
    "            jitter_segments.append(segment)\n",
    "\n",
    "        return jitter_segments\n",
    "\n",
    "    def _rescale_coordinates(self, segments):\n",
    "        # center corner to origin\n",
    "        centered_segments = []\n",
    "        for segment in segments:\n",
    "            segment = segment - np.min(segment, axis=0)\n",
    "            centered_segments.append(segment)\n",
    "        segments = centered_segments\n",
    "\n",
    "        # store the last scaling factors that were used\n",
    "        self.last_scales = []\n",
    "\n",
    "        # rescale coordinates to fit inside voxel matrix\n",
    "        rescaled_segments = []\n",
    "        for segment in segments:\n",
    "            # choose scale\n",
    "            if self.scale_method == \"fixed\":\n",
    "                scale = self.scale\n",
    "                segment = segment / scale * (self.voxels - 1)\n",
    "            elif self.scale_method == \"aspect\":\n",
    "                scale = np.tile(np.max(segment), 3)\n",
    "                segment = segment / scale * (self.voxels - 1)\n",
    "            elif self.scale_method == \"fit\":\n",
    "                scale = np.max(segment, axis=0)\n",
    "                thresholded_scale = np.maximum(scale, self.min_scale)\n",
    "                segment = segment / thresholded_scale * (self.voxels - 1)\n",
    "\n",
    "            # recenter segment\n",
    "            if self.center_method != \"none\":\n",
    "                if self.center_method == \"mean\":\n",
    "                    center = np.mean(segment, axis=0)\n",
    "                elif self.center_method == \"min_max\":\n",
    "                    center = np.max(segment, axis=0) / 2.0\n",
    "\n",
    "                segment = segment + (self.voxels - 1) / 2.0 - center\n",
    "\n",
    "            self.last_scales.append(scale)\n",
    "            rescaled_segments.append(segment)\n",
    "\n",
    "        return rescaled_segments\n",
    "\n",
    "    def _voxelize(self, segments):\n",
    "        voxelized_segments = np.zeros((len(segments),) + tuple(self.voxels))\n",
    "        for i, segment in enumerate(segments):\n",
    "            # remove out of bounds points\n",
    "            segment = segment[np.all(segment < self.voxels, axis=1), :]\n",
    "            segment = segment[np.all(segment >= 0, axis=1), :]\n",
    "\n",
    "            # round coordinates\n",
    "            segment = segment.astype(np.int)\n",
    "\n",
    "            # fill voxel grid\n",
    "            voxelized_segments[i, segment[:, 0], segment[:, 1], segment[:, 2]] = 1\n",
    "\n",
    "        return voxelized_segments\n",
    "\n",
    "    def _train_scaler(self, train_ids):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        scaler = StandardScaler(with_mean=self.remove_mean, with_std=self.remove_std)\n",
    "\n",
    "        for p in range(self.scaler_train_passes):\n",
    "            for i in np.arange(0, len(train_ids), self.batch_size):\n",
    "                segment_ids = train_ids[i : i + self.batch_size]\n",
    "                segments, _ = self.get_processed(segment_ids)\n",
    "                segments = np.reshape(segments, (segments.shape[0], -1))\n",
    "                scaler.partial_fit(segments)\n",
    "\n",
    "        self._scaler = scaler\n",
    "        self._scaler_exists = True\n",
    "\n",
    "    # remove mean and std\n",
    "    def _normalize_voxel_matrix(self, segments):\n",
    "        segments = np.reshape(segments, (segments.shape[0], -1))\n",
    "        segments = self._scaler.transform(segments)\n",
    "        segments = np.reshape(segments, (segments.shape[0],) + tuple(self.voxels))\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def save_scaler(self, path):\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"w\") as fp:\n",
    "            pickle.dump(self._scaler, fp)\n",
    "\n",
    "    def load_scaler(self, path):\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"r\") as fp:\n",
    "            self._scaler = pickle.load(fp)\n",
    "            self._scaler_exists = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset from databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    # load config values\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder=folder,\n",
    "        base_dir=\"\",\n",
    "        require_change=0.0,\n",
    "        use_merges=True,\n",
    "        keep_match_thresh=0.0,\n",
    "        use_matches=True,\n",
    "        min_class_size=1,\n",
    "        require_relevance=0.0,\n",
    "        require_diff_points=0,\n",
    "        normalize_classes=True,\n",
    "    ):\n",
    "        abs_folder = os.path.abspath(os.path.join(base_dir, folder))\n",
    "        try:\n",
    "            assert os.path.isdir(abs_folder)\n",
    "        except AssertionError:\n",
    "            raise IOError(\"Dataset folder {} not found.\".format(abs_folder))\n",
    "\n",
    "        self.folder = abs_folder\n",
    "        self.require_change = require_change\n",
    "        self.use_merges = use_merges\n",
    "        self.keep_match_thresh = keep_match_thresh\n",
    "        self.use_matches = use_matches\n",
    "        self.min_class_size = min_class_size\n",
    "        self.require_relevance = require_relevance\n",
    "        self.require_diff_points = require_diff_points\n",
    "        self.normalize_classes = normalize_classes\n",
    "\n",
    "    # load the segment dataset\n",
    "    def load(self, preprocessor=None):\n",
    "#         from ..tools.import_export import load_segments, load_positions, load_features\n",
    "\n",
    "        # load all the csv files\n",
    "        self.segments, sids, duplicate_sids = load_segments(folder=self.folder)\n",
    "        self.positions, pids, duplicate_pids = load_positions(folder=self.folder)\n",
    "        self.features, self.feature_names, fids, duplicate_fids = load_features(\n",
    "            folder=self.folder\n",
    "        )\n",
    "\n",
    "        self.classes = np.array(sids)\n",
    "        self.duplicate_classes = self.classes.copy()\n",
    "        self.positions = np.array(self.positions)\n",
    "        self.features = np.array(self.features)\n",
    "        self.duplicate_ids = np.array(duplicate_sids)\n",
    "\n",
    "        # load labels\n",
    "#         from ..tools.import_export import load_labels\n",
    "\n",
    "        self.labels, self.lids = load_labels(folder=self.folder)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.labels_dict = dict(zip(self.lids, self.labels))\n",
    "\n",
    "        # load matches\n",
    "#         from ..tools.import_export import load_matches\n",
    "\n",
    "        self.matches = load_matches(folder=self.folder)\n",
    "\n",
    "        if self.require_change > 0.0:\n",
    "            self._remove_unchanged()\n",
    "\n",
    "        # combine sequences that are part of a merger\n",
    "        if self.use_merges:\n",
    "#             from ..tools.import_export import load_merges\n",
    "\n",
    "            merges, _ = load_merges(folder=self.folder)\n",
    "            self._combine_sequences(merges)\n",
    "            self.duplicate_classes = self.classes.copy()\n",
    "\n",
    "        # remove small irrelevant segments\n",
    "        if self.require_relevance > 0:\n",
    "            self._remove_irrelevant()\n",
    "\n",
    "        # only use segments that are different enough\n",
    "        if self.require_diff_points > 0:\n",
    "            assert preprocessor is not None\n",
    "            self._remove_similar(preprocessor)\n",
    "\n",
    "        # combine classes based on matches\n",
    "        if self.use_matches:\n",
    "            self._combine_classes()\n",
    "\n",
    "        # normalize ids and remove small classes\n",
    "        self._normalize_classes()\n",
    "\n",
    "        print(\n",
    "            \"  Found\",\n",
    "            self.n_classes,\n",
    "            \"valid classes with\",\n",
    "            len(self.segments),\n",
    "            \"segments\",\n",
    "        )\n",
    "\n",
    "        self._sort_ids()\n",
    "\n",
    "        return (\n",
    "            self.segments,\n",
    "            self.positions,\n",
    "            self.classes,\n",
    "            self.n_classes,\n",
    "            self.features,\n",
    "            self.matches,\n",
    "            self.labels_dict,\n",
    "        )\n",
    "\n",
    "    def _remove_unchanged(self):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "\n",
    "            prev_size = self.segments[class_ids[0]].shape[0]\n",
    "            for class_id in class_ids[1:]:\n",
    "                size = self.segments[class_id].shape[0]\n",
    "                if size < prev_size * (1.0 + self.require_change):\n",
    "                    keep[class_id] = False\n",
    "                else:\n",
    "                    prev_size = size\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that changed enough\" % len(self.segments))\n",
    "\n",
    "    # list of sequence pairs to merge and correct from the matches table\n",
    "    def _combine_sequences(self, merges):\n",
    "        # calculate the size of each sequence based on the last element\n",
    "        last_sizes = {}\n",
    "        subclasses = {}\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            last_id = class_ids[np.argmax(self.duplicate_ids[class_ids])]\n",
    "            last_sizes[cls] = len(self.segments[last_id])\n",
    "            subclasses[cls] = []\n",
    "\n",
    "        # make merges and keep a list of the merged sequences for each class\n",
    "        for merge in merges:\n",
    "            merge_sequence, target_sequence = merge\n",
    "\n",
    "            merge_ids = np.where(self.classes == merge_sequence)[0]\n",
    "            target_ids = np.where(self.classes == target_sequence)[0]\n",
    "\n",
    "            self.classes[merge_ids] = target_sequence\n",
    "            self.duplicate_ids[target_ids] += merge_ids.size\n",
    "\n",
    "            subclasses[target_sequence].append(merge_sequence)\n",
    "            subclasses[target_sequence] += subclasses[merge_sequence]\n",
    "            del subclasses[merge_sequence]\n",
    "\n",
    "        # calculate how relevant the merges are based on size\n",
    "        relevant = {}\n",
    "        new_class = {}\n",
    "        for main_class in subclasses:\n",
    "            relevant[main_class] = True\n",
    "            new_class[main_class] = main_class\n",
    "\n",
    "            main_size = last_sizes[main_class]\n",
    "            for sub_class in subclasses[main_class]:\n",
    "                new_class[sub_class] = main_class\n",
    "                sub_size = last_sizes[sub_class]\n",
    "                if float(sub_size) / main_size < self.keep_match_thresh:\n",
    "                    relevant[sub_class] = False\n",
    "                else:\n",
    "                    relevant[sub_class] = True\n",
    "\n",
    "        # ignore non-relevant merges and for the relevant merges replace\n",
    "        # the merged class with the new class name\n",
    "        new_matches = []\n",
    "        for match in self.matches:\n",
    "            new_match = []\n",
    "            for cls in match:\n",
    "                if relevant[cls]:\n",
    "                    new_match.append(new_class[cls])\n",
    "\n",
    "            if len(new_match) > 1:\n",
    "                new_matches.append(new_match)\n",
    "\n",
    "        print(\"  Found %d matches that are relevant after merges\" % len(new_matches))\n",
    "\n",
    "        self.matches = new_matches\n",
    "\n",
    "    # combine the classes in a 1d vector of labeled classes based on a 2d\n",
    "    # listing of segments that should share the same class\n",
    "    def _combine_classes(self):\n",
    "        # filtered out non-unique matches\n",
    "        unique_matches = set()\n",
    "        for match in self.matches:\n",
    "            unique_match = []\n",
    "            for cls in match:\n",
    "                if cls not in unique_match:\n",
    "                    unique_match.append(cls)\n",
    "\n",
    "            if len(unique_match) > 1:\n",
    "                unique_match = tuple(sorted(unique_match))\n",
    "                if unique_match not in unique_matches:\n",
    "                    unique_matches.add(unique_match)\n",
    "\n",
    "        unique_matches = [list(match) for match in unique_matches]\n",
    "        print(\"  Found %d matches that are unique\" % len(unique_matches))\n",
    "\n",
    "        # combine matches with classes in common\n",
    "        groups = {}\n",
    "        class_group = {}\n",
    "\n",
    "        for i, cls in enumerate(np.unique(unique_matches)):\n",
    "            groups[i] = [cls]\n",
    "            class_group[cls] = i\n",
    "\n",
    "        for match in unique_matches:\n",
    "            main_group = class_group[match[0]]\n",
    "\n",
    "            for cls in match:\n",
    "                other_group = class_group[cls]\n",
    "                if other_group != main_group:\n",
    "                    for other_class in groups[other_group]:\n",
    "                        if other_class not in groups[main_group]:\n",
    "                            groups[main_group].append(other_class)\n",
    "                            class_group[other_class] = main_group\n",
    "\n",
    "                    del groups[other_group]\n",
    "\n",
    "        self.matches = [groups[i] for i in groups]\n",
    "        print(\"  Found %d matches after grouping\" % len(self.matches))\n",
    "\n",
    "        # combine the sequences into the same class\n",
    "        for match in self.matches:\n",
    "            assert len(match) > 1\n",
    "            for other_class in match[1:]:\n",
    "                self.classes[self.classes == other_class] = match[0]\n",
    "\n",
    "    # make class ids sequential and remove classes that are too small\n",
    "    def _normalize_classes(self):\n",
    "        # mask of segments to keep\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "\n",
    "        # number of classes and current class counter\n",
    "        self.n_classes = 0\n",
    "        for i in np.unique(self.classes):\n",
    "            # find the elements in the class\n",
    "            idx = self.classes == i\n",
    "            if np.sum(idx) >= self.min_class_size:\n",
    "                # if class is large enough keep and relabel\n",
    "                if self.normalize_classes:\n",
    "                    self.classes[idx] = self.n_classes\n",
    "\n",
    "                # found one more class\n",
    "                self.n_classes = self.n_classes + 1\n",
    "            else:\n",
    "                # mark class for removal and delete label information\n",
    "                keep = np.logical_and(keep, np.logical_not(idx))\n",
    "\n",
    "        # remove data on the removed classes\n",
    "        self._trim_data(keep)\n",
    "\n",
    "    # remove segments that are too small compared to the last\n",
    "    # element in the sequence\n",
    "    def _remove_irrelevant(self):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            last_id = class_ids[np.argmax(self.duplicate_ids[class_ids])]\n",
    "            last_size = len(self.segments[last_id])\n",
    "\n",
    "            for class_id in class_ids:\n",
    "                segment_size = len(self.segments[class_id])\n",
    "                if float(segment_size) / last_size < self.require_relevance:\n",
    "                    keep[class_id] = False\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that are relevant\" % len(self.segments))\n",
    "\n",
    "    # remove segments that are too similar based on hamming distance\n",
    "    def _remove_similar(self, preprocessor):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for c in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == c)[0]\n",
    "\n",
    "            # sort duplicates in chronological order\n",
    "            class_ids = class_ids[np.argsort(self.duplicate_ids[class_ids])]\n",
    "\n",
    "            segments_class = [self.segments[i] for i in class_ids]\n",
    "            segments_class = preprocessor._rescale_coordinates(segments_class)\n",
    "            segments_class = preprocessor._voxelize(segments_class)\n",
    "\n",
    "            for i, segment_1 in enumerate(segments_class):\n",
    "                for segment_2 in segments_class[i + 1 :]:\n",
    "                    diff = np.sum(np.abs(segment_1 - segment_2))\n",
    "\n",
    "                    if diff < self.require_diff_points:\n",
    "                        keep[class_ids[i]] = False\n",
    "                        break\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that are dissimilar\" % len(self.segments))\n",
    "\n",
    "    def _sort_ids(self):\n",
    "        ordered_ids = []\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            class_sequences = self.duplicate_classes[class_ids]\n",
    "            unique_sequences = np.unique(class_sequences)\n",
    "\n",
    "            for unique_sequence in unique_sequences:\n",
    "                sequence_ids = np.where(class_sequences == unique_sequence)[0]\n",
    "                sequence_ids = class_ids[sequence_ids]\n",
    "                sequence_frame_ids = self.duplicate_ids[sequence_ids]\n",
    "\n",
    "                # order chronologically according to frame id\n",
    "                sequence_ids = sequence_ids[np.argsort(sequence_frame_ids)]\n",
    "\n",
    "                ordered_ids += sequence_ids.tolist()\n",
    "\n",
    "        ordered_ids = np.array(ordered_ids)\n",
    "\n",
    "        self.segments = [self.segments[i] for i in ordered_ids]\n",
    "        self.classes = self.classes[ordered_ids]\n",
    "\n",
    "        if self.positions.size > 0:\n",
    "            self.positions = self.positions[ordered_ids]\n",
    "        if self.features.size > 0:\n",
    "            self.features = self.features[ordered_ids]\n",
    "\n",
    "        self.duplicate_ids = self.duplicate_ids[ordered_ids]\n",
    "        self.duplicate_classes = self.duplicate_classes[ordered_ids]\n",
    "\n",
    "    # keep only segments and corresponding data where the keep parameter is true\n",
    "    def _trim_data(self, keep):\n",
    "        self.segments = [segment for (k, segment) in zip(keep, self.segments) if k]\n",
    "        self.classes = self.classes[keep]\n",
    "\n",
    "        if self.positions.size > 0:\n",
    "            self.positions = self.positions[keep]\n",
    "        if self.features.size > 0:\n",
    "            self.features = self.features[keep]\n",
    "\n",
    "        self.duplicate_ids = self.duplicate_ids[keep]\n",
    "        self.duplicate_classes = self.duplicate_classes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 28993 segments from 4441 sequences\n",
      "  Found positions for 28993 segments\n",
      "  Found features for 28993 segments \n",
      "  Found labels for 1972 segment ids\n",
      "  Found 453 matches\n",
      "  Found 2251 merge events\n",
      "  Found 453 matches that are relevant after merges\n",
      "  Found 323 matches that are unique\n",
      "  Found 250 matches after grouping\n",
      "  Found 1885 valid classes with 28993 segments\n"
     ]
    }
   ],
   "source": [
    "folder='dataset18'\n",
    "data18=Dataset(folder=folder)\n",
    "segments, _, ids, n_ids, features, matches, labels_dict=data18.load(preprocessor=Preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 45511 segments from 6398 sequences\n",
      "  Found positions for 45511 segments\n",
      "  Found features for 45511 segments \n",
      "  Found labels for 0 segment ids\n",
      "  Found 231 matches\n",
      "  Found 2942 merge events\n",
      "  Found 231 matches that are relevant after merges\n",
      "  Found 188 matches that are unique\n",
      "  Found 162 matches after grouping\n",
      "  Found 3270 valid classes with 45511 segments\n"
     ]
    }
   ],
   "source": [
    "folder='dataset27'\n",
    "data27=Dataset(folder=folder)\n",
    "segments, _, ids, n_ids, features, matches, labels_dict=data27.load(preprocessor=Preprocessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data from dataset 27 (no labels)\n",
    "\n",
    "segments - list of arrays of coordinates with sime objects\n",
    "\n",
    "ids - array with id of element of segments list (several segments represents one object in different time(?))\n",
    "\n",
    "n_ids - amount of objects\n",
    "\n",
    "labels_dict - segment's id:label (0 - other, 1 - car, 2 - building)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and relabelling\n",
    "\n",
    "For relabelling set RELABEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RELABEL = False\n",
    "AUTOWALLS = False\n",
    "CLASS = 1\n",
    "CLASSES = [\"other\", \"car\", \"building\"]\n",
    "\n",
    "lids = data18.lids\n",
    "lids_lookup = dict()\n",
    "for i, lid in enumerate(lids):\n",
    "    lids_lookup[lid] = i\n",
    "labels = data18.labels\n",
    "\n",
    "if RELABEL:\n",
    "    default_label = CLASS\n",
    "    n_objects = np.sum(labels == CLASS)\n",
    "    print(\"There are \" + str(n_objects) + \" \" + CLASSES[CLASS] + \"(s).\")\n",
    "else:\n",
    "    default_label = 0\n",
    "    fp_labels = open( os.path.join(folder, \"labels_database.csv\"), \"a\")\n",
    "    \n",
    "\n",
    "print(\"Default is \" + str(default_label) + \":\" + CLASSES[default_label] + \".\")\n",
    "print(\"Type q and then ENTER to quit.\")\n",
    "for i in range(ids.size):\n",
    "    # skip if it's not the last duplicate\n",
    "    if i + 1 < ids.size and ids[i] == ids[i + 1]:\n",
    "        continue\n",
    "\n",
    "    if RELABEL:\n",
    "        if not ids[i] in lids_lookup or labels[lids_lookup[ids[i]]] != CLASS:\n",
    "            continue\n",
    "#     else:\n",
    "#         if ids[i] in lids_lookup:\n",
    "#             continue\n",
    "\n",
    "    fig = plt.figure(1,figsize=(12,6))\n",
    "    plt.clf()\n",
    "\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "\n",
    "    segment = segments[i]\n",
    "    segment = segment - np.min(segment, axis=0)\n",
    "\n",
    "    # Maintain aspect ratio on xy scale\n",
    "    ax.set_xlim(0, np.max(segment[:, :]))\n",
    "    ax.set_ylim(0, np.max(segment[:, :]))\n",
    "    ax.set_zlim(0, np.max(segment[:, :]))\n",
    "\n",
    "    x, y, z = np.hsplit(segment, segment.shape[1])\n",
    "    ax.scatter(x, y, z, c=list(((z-min(z))/max(z)).reshape(-1,)))\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlim(0, np.max(segment[:, :]))\n",
    "    ax.set_ylim(0, np.max(segment[:, :]))\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "    print(CLASSES[labels_dict[ids[i]]])\n",
    "\n",
    "    if not RELABEL: \n",
    "        continue\n",
    "    while True:\n",
    "        # autolabel\n",
    "        if AUTOWALLS and not RELABEL:\n",
    "            max_x = max(segment[:, 0])\n",
    "            min_x = min(segment[:, 0])\n",
    "            max_y = max(segment[:, 1])\n",
    "            min_y = min(segment[:, 1])\n",
    "\n",
    "            dist = np.linalg.norm([max_x - min_x, max_y - min_y])\n",
    "\n",
    "            if dist > 6:\n",
    "                print(str(ids[i]) + \" autolabeled as wall\")\n",
    "                label = 2\n",
    "                break\n",
    "\n",
    "        # consider user input\n",
    "        label = input(str(ids[i]) + \" label: \")\n",
    "\n",
    "        if not label:\n",
    "            label = default_label\n",
    "            break\n",
    "        if label in [\"0\", \"1\", \"2\", \"q\"]:\n",
    "            break\n",
    "\n",
    "    if label == \"q\":\n",
    "        break\n",
    "\n",
    "    if RELABEL:\n",
    "        labels[lids_lookup[ids[i]]] = label\n",
    "    else:\n",
    "        fp_labels.write(str(ids[i]) + \" \" + str(label) + \"\\n\")\n",
    "        fp_labels.flush()\n",
    "\n",
    "if RELABEL:\n",
    "    fp_labels = open(os.path.join(folder, \"labels_database.csv\"), \"w\")\n",
    "    for lid, label in zip(lids, labels):\n",
    "        fp_labels.write(str(lid) + \" \" + str(label) + \"\\n\")\n",
    "fp_labels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset27 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "RELABEL = False\n",
    "AUTOWALLS = False\n",
    "CLASS = 0\n",
    "CLASSES = [\"other\", \"car\", \"building\"]\n",
    "\n",
    "lids = data27.lids\n",
    "lids_lookup = dict()\n",
    "for i, lid in enumerate(lids):\n",
    "    lids_lookup[lid] = i\n",
    "labels = data27.labels\n",
    "\n",
    "if RELABEL:\n",
    "    default_label = CLASS\n",
    "    n_objects = np.sum(labels == CLASS)\n",
    "    print(\"There are \" + str(n_objects) + \" \" + CLASSES[CLASS] + \"(s).\")\n",
    "else:\n",
    "    default_label = 0\n",
    "    fp_labels = open( os.path.join(folder, \"labels_database.csv\"), \"a\")\n",
    "    \n",
    "\n",
    "print(\"Default is \" + str(default_label) + \":\" + CLASSES[default_label] + \".\")\n",
    "print(\"Type q and then ENTER to quit.\")\n",
    "for i in range(ids.size):\n",
    "    # skip if it's not the last duplicate\n",
    "    if i + 1 < ids.size and ids[i] == ids[i + 1]:\n",
    "        continue\n",
    "\n",
    "    if RELABEL:\n",
    "        if not ids[i] in lids_lookup or labels[lids_lookup[ids[i]]] != CLASS:\n",
    "            continue\n",
    "    else:\n",
    "        if ids[i] in lids_lookup:\n",
    "            continue\n",
    "\n",
    "    fig = plt.figure(1,figsize=(12,6))\n",
    "    plt.clf()\n",
    "\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "\n",
    "    segment = segments[i]\n",
    "    segment = segment - np.min(segment, axis=0)\n",
    "\n",
    "    # Maintain aspect ratio on xy scale\n",
    "    ax.set_xlim(0, np.max(segment[:, :]))\n",
    "    ax.set_ylim(0, np.max(segment[:, :]))\n",
    "    ax.set_zlim(0, np.max(segment[:, :]))\n",
    "\n",
    "    x, y, z = np.hsplit(segment, segment.shape[1])\n",
    "    ax.scatter(x, y, z, c=list(((z-min(z))/max(z)).reshape(-1,)))\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlim(0, np.max(segment[:, :]))\n",
    "    ax.set_ylim(0, np.max(segment[:, :]))\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "#     print(CLASSES[labels_dict[ids[i]]])\n",
    "\n",
    "    if not RELABEL: \n",
    "        continue\n",
    "    while True:\n",
    "        # autolabel\n",
    "        if AUTOWALLS and not RELABEL:\n",
    "            max_x = max(segment[:, 0])\n",
    "            min_x = min(segment[:, 0])\n",
    "            max_y = max(segment[:, 1])\n",
    "            min_y = min(segment[:, 1])\n",
    "\n",
    "            dist = np.linalg.norm([max_x - min_x, max_y - min_y])\n",
    "\n",
    "            if dist > 6:\n",
    "                print(str(ids[i]) + \" autolabeled as wall\")\n",
    "                label = 2\n",
    "                break\n",
    "\n",
    "        # consider user input\n",
    "        label = input(str(ids[i]) + \" label: \")\n",
    "\n",
    "        if not label:\n",
    "            label = default_label\n",
    "            break\n",
    "        if label in [\"0\", \"1\", \"2\", \"q\"]:\n",
    "            break\n",
    "\n",
    "    if label == \"q\":\n",
    "        break\n",
    "\n",
    "    if RELABEL:\n",
    "        labels[lids_lookup[ids[i]]] = label\n",
    "    else:\n",
    "        fp_labels.write(str(ids[i]) + \" \" + str(label) + \"\\n\")\n",
    "        fp_labels.flush()\n",
    "\n",
    "if RELABEL:\n",
    "    fp_labels = open(os.path.join(folder, \"labels_database.csv\"), \"w\")\n",
    "    for lid, label in zip(lids, labels):\n",
    "        fp_labels.write(str(lid) + \" \" + str(label) + \"\\n\")\n",
    "fp_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linearity',\n",
       " 'planarity',\n",
       " 'scattering',\n",
       " 'omnivariance',\n",
       " 'anisotropy',\n",
       " 'eigen_entropy',\n",
       " 'change_of_curvature',\n",
       " 'pointing_up',\n",
       " 'cnn_0',\n",
       " 'cnn_1',\n",
       " 'cnn_2',\n",
       " 'cnn_3',\n",
       " 'cnn_4',\n",
       " 'cnn_5',\n",
       " 'cnn_6',\n",
       " 'cnn_7',\n",
       " 'cnn_8',\n",
       " 'cnn_9',\n",
       " 'cnn_10',\n",
       " 'cnn_11',\n",
       " 'cnn_12',\n",
       " 'cnn_13',\n",
       " 'cnn_14',\n",
       " 'cnn_15',\n",
       " 'cnn_16',\n",
       " 'cnn_17',\n",
       " 'cnn_18',\n",
       " 'cnn_19',\n",
       " 'cnn_20',\n",
       " 'cnn_21',\n",
       " 'cnn_22',\n",
       " 'cnn_23',\n",
       " 'cnn_24',\n",
       " 'cnn_25',\n",
       " 'cnn_26',\n",
       " 'cnn_27',\n",
       " 'cnn_28',\n",
       " 'cnn_29',\n",
       " 'cnn_30',\n",
       " 'cnn_31',\n",
       " 'cnn_scale_x',\n",
       " 'cnn_scale_y',\n",
       " 'cnn_scale_z']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
