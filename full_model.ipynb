{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder='C:/Users/kichk/Downloads/segmap_machine_learning_project-master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_folders = folder\n",
    "cnn_test_folder = folder \n",
    "semantics_train_folder = folder\n",
    "normalize_classes = True\n",
    "# Combine sequences based on merge events triggered in segmatch\n",
    "use_merges = True\n",
    "\n",
    "# Size of the merged sequence compared to the last element in the merged\n",
    "# sequence to keep matches containing the merged sequence\n",
    "keep_match_thresh = 0.3\n",
    "\n",
    "# Combine the views based on the segmatch matches\n",
    "use_matches = True\n",
    "\n",
    "# Discard classes of segments that are smaller than min_class_size\n",
    "min_class_size = 2\n",
    "\n",
    "# The relative size of a segment compared to the last segment in the sequence\n",
    "# so that it is still considered relevant and kept\n",
    "require_relevance = 0.05\n",
    "\n",
    "# The number of points that must be different so that two segments are\n",
    "# considered different. Similar segments are removed in chronological order\n",
    "require_diff_points = 0\n",
    "\n",
    "# Generate new samples by randomly rotating each sample by\n",
    "# [-augment_angle, augment_angle] degrees.\n",
    "augment_angle = 180\n",
    "require_change=0.1\n",
    "# Augment by randomly removing a percentage of points from each sample\n",
    "augment_remove_random_min = 0.0\n",
    "augment_remove_random_max = 0.1\n",
    "augment_remove_plane_min = 0.0\n",
    "augment_remove_plane_max = 0.5\n",
    "\n",
    "# Augment by randomly jittering the segment after centering\n",
    "augment_jitter = 0.0\n",
    "\n",
    "# Align the segments (robot/eigen/none)\n",
    "align = \"eigen\"\n",
    "\n",
    "# Which type of scaling to use\n",
    "#     - fixed: use a fixed scale\n",
    "#     - aspect: scale, but maintain aspect ratio\n",
    "#     - fit: scale each dimenstion indipendently\n",
    "scale_method = \"fit\"\n",
    "\n",
    "# How to center the segment\n",
    "#     - mean: based on the segments mean, some point will be out of bounds\n",
    "#     - min_max: centers based on the min and max of each dimension\n",
    "#     - none: no centering\n",
    "center_method = \"mean\"\n",
    "\n",
    "# Size of the voxel parallelepiped in meters\n",
    "scale_x = 8\n",
    "scale_y = 8\n",
    "scale_z = 4\n",
    "\n",
    "# Number of voxels in the rectangular parallelepiped into\n",
    "# which to normalize each segment\n",
    "voxels_x = 32\n",
    "voxels_y = 32\n",
    "voxels_z = 16\n",
    "\n",
    "# Remove the mean and std\n",
    "remove_mean = False\n",
    "remove_std = False\n",
    "\n",
    "# Folder into which to save the model after training\n",
    "# If no model_base_dir is given, the default model_base_dir will be used instead\n",
    "#model_base_dir = ...\n",
    "cnn_model_folder = \"/segmap64\"\n",
    "semantics_model_folder = \"/segmap64_semantics\"\n",
    "\n",
    "# Percentage of match sequences to put in the test set\n",
    "test_size = 0.3\n",
    "\n",
    "# Number of epochs to train for\n",
    "n_epochs = 6\n",
    "checkpoints = 3\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Root path to save tensorboard logs\n",
    "log_path = \"/tmp/tensorboard/\"\n",
    "\n",
    "# Directory where to save debug outputs\n",
    "debug = False\n",
    "debug_path = \"/tmp/debug\"\n",
    "retrain = 'C:/Users/kichk/Downloads/segmap_machine_learning_project-master/segmap64/model.ckpt'\n",
    "keep_best = False\n",
    "roc = True \n",
    "log_name = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        augment_angle=0.0,\n",
    "        augment_remove_random_min=0.0,\n",
    "        augment_remove_random_max=0.0,\n",
    "        augment_remove_plane_min=0.0,\n",
    "        augment_remove_plane_max=0.0,\n",
    "        augment_jitter=0.0,\n",
    "        align=\"none\",\n",
    "        voxelize=True,\n",
    "        scale_method=\"fixed\",\n",
    "        center_method=\"none\",\n",
    "        scale=(1, 1, 1),\n",
    "        voxels=(1, 1, 1),\n",
    "        remove_mean=False,\n",
    "        remove_std=False,\n",
    "        batch_size=16,\n",
    "        scaler_train_passes=1,\n",
    "    ):\n",
    "        self.augment_remove_random_min = augment_remove_random_min\n",
    "        self.augment_remove_random_max = augment_remove_random_max\n",
    "        self.augment_remove_plane_min = augment_remove_plane_min\n",
    "        self.augment_remove_plane_max = augment_remove_plane_max\n",
    "        self.augment_angle = augment_angle\n",
    "        self.augment_jitter = augment_jitter\n",
    "\n",
    "        self.align = align\n",
    "        self.voxelize = voxelize\n",
    "        self.scale_method = scale_method\n",
    "        self.center_method = center_method\n",
    "        self.scale = np.array(scale)\n",
    "        self.voxels = np.array(voxels)\n",
    "        self.remove_mean = remove_mean\n",
    "        self.remove_std = remove_std\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler_train_passes = scaler_train_passes\n",
    "        self._scaler_exists = False\n",
    "\n",
    "        min_voxel_side_length_m = 0.1\n",
    "        self.min_scale = self.voxels * min_voxel_side_length_m\n",
    "\n",
    "        self.last_scales = []\n",
    "\n",
    "    def init_segments(\n",
    "        self, segments, classes, positions=None, train_ids=None, scaler_path=None\n",
    "    ):\n",
    "\n",
    "        self.segments = segments\n",
    "        self.classes = np.array(classes)\n",
    "\n",
    "        if self.align == \"robot\":\n",
    "            assert positions is not None\n",
    "            self.segments = self._align_robot(self.segments, positions)\n",
    "\n",
    "        # check if we need to train a scaler\n",
    "        if self.remove_mean or self.remove_std:\n",
    "            if scaler_path is None:\n",
    "                assert train_ids is not None\n",
    "                self._train_scaler(train_ids)\n",
    "            else:\n",
    "                self.load_scaler(scaler_path)\n",
    "\n",
    "    def get_processed(self, segment_ids, train=True, normalize=True):\n",
    "        batch_segments = []\n",
    "        for i in segment_ids:\n",
    "            batch_segments.append(self.segments[i])\n",
    "\n",
    "        batch_segments = self.process(batch_segments, train, normalize)\n",
    "        batch_classes = self.classes[segment_ids]\n",
    "\n",
    "        return batch_segments, batch_classes\n",
    "\n",
    "    def process(self, segments, train=True, normalize=True):\n",
    "        # augment through distorsions\n",
    "        if train and self.augment_remove_random_max > 0:\n",
    "            segments = self._augment_remove_random(segments)\n",
    "\n",
    "        if train and self.augment_remove_plane_max > 0:\n",
    "            segments = self._augment_remove_plane(segments)\n",
    "\n",
    "        # align after distorsions\n",
    "        if self.align == \"eigen\":\n",
    "            segments = self._align_eigen(segments)\n",
    "\n",
    "        # augment rotation\n",
    "        if train and self.augment_angle > 0:\n",
    "            segments = self._augment_rotation(segments)\n",
    "\n",
    "        if self.voxelize:\n",
    "            # rescale coordinates and center\n",
    "            segments = self._rescale_coordinates(segments)\n",
    "\n",
    "            # randomly displace the segment\n",
    "            if train and self.augment_jitter > 0:\n",
    "                segments = self._augment_jitter(segments)\n",
    "\n",
    "            # insert into voxel grid\n",
    "            segments = self._voxelize(segments)\n",
    "\n",
    "            # remove mean and/or std\n",
    "            if normalize and self._scaler_exists:\n",
    "                segments = self._normalize_voxel_matrix(segments)\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def get_n_batches(self, train=True):\n",
    "        if train:\n",
    "            return self.n_batches_train\n",
    "        else:\n",
    "            return self.n_batches_test\n",
    "\n",
    "    # create rotation matrix that rotates point around\n",
    "    # the origin by an angle theta, expressed in radians\n",
    "    def _get_rotation_matrix_z(self, theta):\n",
    "        R_z = [\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "\n",
    "        return np.array(R_z)\n",
    "\n",
    "    # align according to the robot's position\n",
    "    def _align_robot(self, segments, positions):\n",
    "        aligned_segments = []\n",
    "        for i, seg in enumerate(segments):\n",
    "            center = np.mean(seg, axis=0)\n",
    "\n",
    "            robot_pos = positions[i] - center\n",
    "            seg = seg - center\n",
    "\n",
    "            # angle between robot and x-axis\n",
    "            angle = np.arctan2(robot_pos[1], robot_pos[0])\n",
    "\n",
    "            # align the segment so the robots perspective is along the x-axis\n",
    "            inv_rotation_matrix_z = self._get_rotation_matrix_z(angle)\n",
    "            aligned_seg = np.dot(seg, inv_rotation_matrix_z)\n",
    "\n",
    "            aligned_segments.append(aligned_seg)\n",
    "\n",
    "        return aligned_segments\n",
    "\n",
    "    def _align_eigen(self, segments):\n",
    "        aligned_segments = []\n",
    "        for segment in segments:\n",
    "            # Calculate covariance\n",
    "            center = np.mean(segment, axis=0)\n",
    "\n",
    "            covariance_2d = np.cov(segment[:, :2] - center[:2], rowvar=False, bias=True)\n",
    "\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(covariance_2d)\n",
    "            alignment_rad = np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0])\n",
    "\n",
    "            if eigenvalues[0] < eigenvalues[1]:\n",
    "                alignment_rad = alignment_rad + np.pi / 2\n",
    "\n",
    "            inv_rotation_matrix_z = self._get_rotation_matrix_z(alignment_rad)\n",
    "            aligned_segment = np.dot(segment, inv_rotation_matrix_z)\n",
    "\n",
    "            y_center = np.mean(segment[:, 1])\n",
    "            n_below = np.sum(segment[:, 1] < y_center)\n",
    "\n",
    "            if n_below < segment.shape[0] / 2:\n",
    "                alignment_rad = alignment_rad + np.pi\n",
    "                inv_rotation_matrix_z = self._get_rotation_matrix_z(np.pi)\n",
    "                aligned_segment = np.dot(aligned_segment, inv_rotation_matrix_z)\n",
    "\n",
    "            aligned_segments.append(aligned_segment)\n",
    "\n",
    "        return aligned_segments\n",
    "\n",
    "    # augment with multiple rotation of the same segment\n",
    "    def _augment_rotation(self, segments):\n",
    "        angle_rad = self.augment_angle * np.pi / 180\n",
    "\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            rotation = np.random.uniform(-angle_rad, angle_rad)\n",
    "            segment = np.dot(segment, self._get_rotation_matrix_z(rotation))\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_remove_random(self, segments):\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            # percentage of points to remove\n",
    "            remove = (\n",
    "                np.random.random()\n",
    "                * (self.augment_remove_random_max - self.augment_remove_random_min)\n",
    "                + self.augment_remove_random_min\n",
    "            )\n",
    "\n",
    "            # randomly choose the points\n",
    "            idx = np.arange(segment.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            idx = idx[int(idx.size * remove) :]\n",
    "\n",
    "            segment = segment[idx]\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_remove_plane(self, segments):\n",
    "        augmented_segments = []\n",
    "        for segment in segments:\n",
    "            # center segment\n",
    "            center = np.mean(segment, axis=0)\n",
    "            segment = segment - center\n",
    "\n",
    "            # slice off a section of the segment\n",
    "            while True:\n",
    "                # generate random plane\n",
    "                plane_norm = np.random.random(3) - 0.5\n",
    "                plane_norm = plane_norm / np.sqrt(np.sum(plane_norm ** 2))\n",
    "\n",
    "                # on which side of the plane each point is\n",
    "                sign = np.dot(segment, plane_norm)\n",
    "\n",
    "                # find an offset that removes a desired amount of points\n",
    "                found = False\n",
    "                plane_offsets = np.linspace(\n",
    "                    -np.max(self.scale), np.max(self.scale), 100\n",
    "                )\n",
    "                np.random.shuffle(plane_offsets)\n",
    "                for plane_offset in plane_offsets:\n",
    "                    keep = sign + plane_offset > 0\n",
    "                    remove_percentage = 1 - (np.sum(keep) / float(keep.size))\n",
    "\n",
    "                    if (\n",
    "                        remove_percentage > self.augment_remove_plane_min\n",
    "                        and remove_percentage < self.augment_remove_plane_max\n",
    "                    ):\n",
    "                        segment = segment[keep]\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "            segment = segment + center\n",
    "            augmented_segments.append(segment)\n",
    "\n",
    "        return augmented_segments\n",
    "\n",
    "    def _augment_jitter(self, segments):\n",
    "        jitter_segments = []\n",
    "        for segment in segments:\n",
    "            jitter = np.random.random(3) * 2 - 1\n",
    "            jitter = jitter * self.augment_jitter * self.voxels / 2\n",
    "\n",
    "            segment = segment + jitter\n",
    "            jitter_segments.append(segment)\n",
    "\n",
    "        return jitter_segments\n",
    "\n",
    "    def _rescale_coordinates(self, segments):\n",
    "        # center corner to origin\n",
    "        centered_segments = []\n",
    "        for segment in segments:\n",
    "            segment = segment - np.min(segment, axis=0)\n",
    "            centered_segments.append(segment)\n",
    "        segments = centered_segments\n",
    "\n",
    "        # store the last scaling factors that were used\n",
    "        self.last_scales = []\n",
    "\n",
    "        # rescale coordinates to fit inside voxel matrix\n",
    "        rescaled_segments = []\n",
    "        for segment in segments:\n",
    "            # choose scale\n",
    "            if self.scale_method == \"fixed\":\n",
    "                scale = self.scale\n",
    "                segment = segment / scale * (self.voxels - 1)\n",
    "            elif self.scale_method == \"aspect\":\n",
    "                scale = np.tile(np.max(segment), 3)\n",
    "                segment = segment / scale * (self.voxels - 1)\n",
    "            elif self.scale_method == \"fit\":\n",
    "                scale = np.max(segment, axis=0)\n",
    "                thresholded_scale = np.maximum(scale, self.min_scale)\n",
    "                segment = segment / thresholded_scale * (self.voxels - 1)\n",
    "\n",
    "            # recenter segment\n",
    "            if self.center_method != \"none\":\n",
    "                if self.center_method == \"mean\":\n",
    "                    center = np.mean(segment, axis=0)\n",
    "                elif self.center_method == \"min_max\":\n",
    "                    center = np.max(segment, axis=0) / 2.0\n",
    "\n",
    "                segment = segment + (self.voxels - 1) / 2.0 - center\n",
    "\n",
    "            self.last_scales.append(scale)\n",
    "            rescaled_segments.append(segment)\n",
    "\n",
    "        return rescaled_segments\n",
    "\n",
    "    def _voxelize(self, segments):\n",
    "        voxelized_segments = np.zeros((len(segments),) + tuple(self.voxels))\n",
    "        for i, segment in enumerate(segments):\n",
    "            # remove out of bounds points\n",
    "            segment = segment[np.all(segment < self.voxels, axis=1), :]\n",
    "            segment = segment[np.all(segment >= 0, axis=1), :]\n",
    "\n",
    "            # round coordinates\n",
    "            segment = segment.astype(np.int)\n",
    "\n",
    "            # fill voxel grid\n",
    "            voxelized_segments[i, segment[:, 0], segment[:, 1], segment[:, 2]] = 1\n",
    "\n",
    "        return voxelized_segments\n",
    "\n",
    "    def _train_scaler(self, train_ids):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        scaler = StandardScaler(with_mean=self.remove_mean, with_std=self.remove_std)\n",
    "\n",
    "        for p in range(self.scaler_train_passes):\n",
    "            for i in np.arange(0, len(train_ids), self.batch_size):\n",
    "                segment_ids = train_ids[i : i + self.batch_size]\n",
    "                segments, _ = self.get_processed(segment_ids)\n",
    "                segments = np.reshape(segments, (segments.shape[0], -1))\n",
    "                scaler.partial_fit(segments)\n",
    "\n",
    "        self._scaler = scaler\n",
    "        self._scaler_exists = True\n",
    "\n",
    "    # remove mean and std\n",
    "    def _normalize_voxel_matrix(self, segments):\n",
    "        segments = np.reshape(segments, (segments.shape[0], -1))\n",
    "        segments = self._scaler.transform(segments)\n",
    "        segments = np.reshape(segments, (segments.shape[0],) + tuple(self.voxels))\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def save_scaler(self, path):\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"w\") as fp:\n",
    "            pickle.dump(self._scaler, fp)\n",
    "\n",
    "    def load_scaler(self, path):\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"r\") as fp:\n",
    "            self._scaler = pickle.load(fp)\n",
    "            self._scaler_exists = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    # load config values\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder=folder,\n",
    "        base_dir=\"\",\n",
    "        require_change=require_change,\n",
    "        use_merges=use_merges,\n",
    "        keep_match_thresh=keep_match_thresh,\n",
    "        use_matches=use_matches,\n",
    "        min_class_size=min_class_size,\n",
    "        require_relevance=require_relevance,\n",
    "        require_diff_points=require_diff_points,\n",
    "        normalize_classes=normalize_classes,\n",
    "    ):\n",
    "        abs_folder = os.path.abspath(os.path.join(base_dir, folder))\n",
    "        try:\n",
    "            assert os.path.isdir(abs_folder)\n",
    "        except AssertionError:\n",
    "            raise IOError(\"Dataset folder {} not found.\".format(abs_folder))\n",
    "\n",
    "        self.folder = abs_folder\n",
    "        self.require_change = require_change\n",
    "        self.use_merges = use_merges\n",
    "        self.keep_match_thresh = keep_match_thresh\n",
    "        self.use_matches = use_matches\n",
    "        self.min_class_size = min_class_size\n",
    "        self.require_relevance = require_relevance\n",
    "        self.require_diff_points = require_diff_points\n",
    "        self.normalize_classes = normalize_classes\n",
    "\n",
    "    # load the segment dataset\n",
    "    def load(self, preprocessor=None):\n",
    "#         from ..tools.import_export import load_segments, load_positions, load_features\n",
    "\n",
    "        # load all the csv files\n",
    "        self.segments, sids, duplicate_sids = load_segments(folder=self.folder)\n",
    "        self.positions, pids, duplicate_pids = load_positions(folder=self.folder)\n",
    "        self.features, self.feature_names, fids, duplicate_fids = load_features(\n",
    "            folder=self.folder\n",
    "        )\n",
    "\n",
    "        self.classes = np.array(sids)\n",
    "        self.duplicate_classes = self.classes.copy()\n",
    "        self.positions = np.array(self.positions)\n",
    "        self.features = np.array(self.features)\n",
    "        self.duplicate_ids = np.array(duplicate_sids)\n",
    "\n",
    "        # load labels\n",
    "#         from ..tools.import_export import load_labels\n",
    "\n",
    "        self.labels, self.lids = load_labels(folder=self.folder)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.labels_dict = dict(zip(self.lids, self.labels))\n",
    "\n",
    "        # load matches\n",
    "#         from ..tools.import_export import load_matches\n",
    "\n",
    "        self.matches = load_matches(folder=self.folder)\n",
    "\n",
    "        if self.require_change > 0.0:\n",
    "            self._remove_unchanged()\n",
    "\n",
    "        # combine sequences that are part of a merger\n",
    "        if self.use_merges:\n",
    "#             from ..tools.import_export import load_merges\n",
    "\n",
    "            merges, _ = load_merges(folder=self.folder)\n",
    "            self._combine_sequences(merges)\n",
    "            self.duplicate_classes = self.classes.copy()\n",
    "\n",
    "        # remove small irrelevant segments\n",
    "        if self.require_relevance > 0:\n",
    "            self._remove_irrelevant()\n",
    "\n",
    "        # only use segments that are different enough\n",
    "        if self.require_diff_points > 0:\n",
    "            assert preprocessor is not None\n",
    "            self._remove_similar(preprocessor)\n",
    "\n",
    "        # combine classes based on matches\n",
    "        if self.use_matches:\n",
    "            self._combine_classes()\n",
    "\n",
    "        # normalize ids and remove small classes\n",
    "        self._normalize_classes()\n",
    "\n",
    "        print(\n",
    "            \"  Found\",\n",
    "            self.n_classes,\n",
    "            \"valid classes with\",\n",
    "            len(self.segments),\n",
    "            \"segments\",\n",
    "        )\n",
    "\n",
    "        self._sort_ids()\n",
    "\n",
    "        return (\n",
    "            self.segments,\n",
    "            self.positions,\n",
    "            self.classes,\n",
    "            self.n_classes,\n",
    "            self.features,\n",
    "            self.matches,\n",
    "            self.labels_dict,\n",
    "        )\n",
    "\n",
    "    def _remove_unchanged(self):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "\n",
    "            prev_size = self.segments[class_ids[0]].shape[0]\n",
    "            for class_id in class_ids[1:]:\n",
    "                size = self.segments[class_id].shape[0]\n",
    "                if size < prev_size * (1.0 + self.require_change):\n",
    "                    keep[class_id] = False\n",
    "                else:\n",
    "                    prev_size = size\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that changed enough\" % len(self.segments))\n",
    "\n",
    "    # list of sequence pairs to merge and correct from the matches table\n",
    "    def _combine_sequences(self, merges):\n",
    "        # calculate the size of each sequence based on the last element\n",
    "        last_sizes = {}\n",
    "        subclasses = {}\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            last_id = class_ids[np.argmax(self.duplicate_ids[class_ids])]\n",
    "            last_sizes[cls] = len(self.segments[last_id])\n",
    "            subclasses[cls] = []\n",
    "\n",
    "        # make merges and keep a list of the merged sequences for each class\n",
    "        for merge in merges:\n",
    "            merge_sequence, target_sequence = merge\n",
    "\n",
    "            merge_ids = np.where(self.classes == merge_sequence)[0]\n",
    "            target_ids = np.where(self.classes == target_sequence)[0]\n",
    "\n",
    "            self.classes[merge_ids] = target_sequence\n",
    "            self.duplicate_ids[target_ids] += merge_ids.size\n",
    "\n",
    "            subclasses[target_sequence].append(merge_sequence)\n",
    "            subclasses[target_sequence] += subclasses[merge_sequence]\n",
    "            del subclasses[merge_sequence]\n",
    "\n",
    "        # calculate how relevant the merges are based on size\n",
    "        relevant = {}\n",
    "        new_class = {}\n",
    "        for main_class in subclasses:\n",
    "            relevant[main_class] = True\n",
    "            new_class[main_class] = main_class\n",
    "\n",
    "            main_size = last_sizes[main_class]\n",
    "            for sub_class in subclasses[main_class]:\n",
    "                new_class[sub_class] = main_class\n",
    "                sub_size = last_sizes[sub_class]\n",
    "                if float(sub_size) / main_size < self.keep_match_thresh:\n",
    "                    relevant[sub_class] = False\n",
    "                else:\n",
    "                    relevant[sub_class] = True\n",
    "\n",
    "        # ignore non-relevant merges and for the relevant merges replace\n",
    "        # the merged class with the new class name\n",
    "        new_matches = []\n",
    "        for match in self.matches:\n",
    "            new_match = []\n",
    "            for cls in match:\n",
    "                if relevant[cls]:\n",
    "                    new_match.append(new_class[cls])\n",
    "\n",
    "            if len(new_match) > 1:\n",
    "                new_matches.append(new_match)\n",
    "\n",
    "        print(\"  Found %d matches that are relevant after merges\" % len(new_matches))\n",
    "\n",
    "        self.matches = new_matches\n",
    "\n",
    "    # combine the classes in a 1d vector of labeled classes based on a 2d\n",
    "    # listing of segments that should share the same class\n",
    "    def _combine_classes(self):\n",
    "        # filtered out non-unique matches\n",
    "        unique_matches = set()\n",
    "        for match in self.matches:\n",
    "            unique_match = []\n",
    "            for cls in match:\n",
    "                if cls not in unique_match:\n",
    "                    unique_match.append(cls)\n",
    "\n",
    "            if len(unique_match) > 1:\n",
    "                unique_match = tuple(sorted(unique_match))\n",
    "                if unique_match not in unique_matches:\n",
    "                    unique_matches.add(unique_match)\n",
    "\n",
    "        unique_matches = [list(match) for match in unique_matches]\n",
    "        print(\"  Found %d matches that are unique\" % len(unique_matches))\n",
    "\n",
    "        # combine matches with classes in common\n",
    "        groups = {}\n",
    "        class_group = {}\n",
    "\n",
    "        for i, cls in enumerate(np.unique(unique_matches)):\n",
    "            groups[i] = [cls]\n",
    "            class_group[cls] = i\n",
    "\n",
    "        for match in unique_matches:\n",
    "            main_group = class_group[match[0]]\n",
    "\n",
    "            for cls in match:\n",
    "                other_group = class_group[cls]\n",
    "                if other_group != main_group:\n",
    "                    for other_class in groups[other_group]:\n",
    "                        if other_class not in groups[main_group]:\n",
    "                            groups[main_group].append(other_class)\n",
    "                            class_group[other_class] = main_group\n",
    "\n",
    "                    del groups[other_group]\n",
    "\n",
    "        self.matches = [groups[i] for i in groups]\n",
    "        print(\"  Found %d matches after grouping\" % len(self.matches))\n",
    "\n",
    "        # combine the sequences into the same class\n",
    "        for match in self.matches:\n",
    "            assert len(match) > 1\n",
    "            for other_class in match[1:]:\n",
    "                self.classes[self.classes == other_class] = match[0]\n",
    "\n",
    "    # make class ids sequential and remove classes that are too small\n",
    "    def _normalize_classes(self):\n",
    "        # mask of segments to keep\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "\n",
    "        # number of classes and current class counter\n",
    "        self.n_classes = 0\n",
    "        for i in np.unique(self.classes):\n",
    "            # find the elements in the class\n",
    "            idx = self.classes == i\n",
    "            if np.sum(idx) >= self.min_class_size:\n",
    "                # if class is large enough keep and relabel\n",
    "                if self.normalize_classes:\n",
    "                    self.classes[idx] = self.n_classes\n",
    "\n",
    "                # found one more class\n",
    "                self.n_classes = self.n_classes + 1\n",
    "            else:\n",
    "                # mark class for removal and delete label information\n",
    "                keep = np.logical_and(keep, np.logical_not(idx))\n",
    "\n",
    "        # remove data on the removed classes\n",
    "        self._trim_data(keep)\n",
    "\n",
    "    # remove segments that are too small compared to the last\n",
    "    # element in the sequence\n",
    "    def _remove_irrelevant(self):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            last_id = class_ids[np.argmax(self.duplicate_ids[class_ids])]\n",
    "            last_size = len(self.segments[last_id])\n",
    "\n",
    "            for class_id in class_ids:\n",
    "                segment_size = len(self.segments[class_id])\n",
    "                if float(segment_size) / last_size < self.require_relevance:\n",
    "                    keep[class_id] = False\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that are relevant\" % len(self.segments))\n",
    "\n",
    "    # remove segments that are too similar based on hamming distance\n",
    "    def _remove_similar(self, preprocessor):\n",
    "        keep = np.ones(self.classes.size).astype(np.bool)\n",
    "        for c in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == c)[0]\n",
    "\n",
    "            # sort duplicates in chronological order\n",
    "            class_ids = class_ids[np.argsort(self.duplicate_ids[class_ids])]\n",
    "\n",
    "            segments_class = [self.segments[i] for i in class_ids]\n",
    "            segments_class = preprocessor._rescale_coordinates(segments_class)\n",
    "            segments_class = preprocessor._voxelize(segments_class)\n",
    "\n",
    "            for i, segment_1 in enumerate(segments_class):\n",
    "                for segment_2 in segments_class[i + 1 :]:\n",
    "                    diff = np.sum(np.abs(segment_1 - segment_2))\n",
    "\n",
    "                    if diff < self.require_diff_points:\n",
    "                        keep[class_ids[i]] = False\n",
    "                        break\n",
    "\n",
    "        self._trim_data(keep)\n",
    "\n",
    "        print(\"  Found %d segments that are dissimilar\" % len(self.segments))\n",
    "\n",
    "    def _sort_ids(self):\n",
    "        ordered_ids = []\n",
    "        for cls in np.unique(self.classes):\n",
    "            class_ids = np.where(self.classes == cls)[0]\n",
    "            class_sequences = self.duplicate_classes[class_ids]\n",
    "            unique_sequences = np.unique(class_sequences)\n",
    "\n",
    "            for unique_sequence in unique_sequences:\n",
    "                sequence_ids = np.where(class_sequences == unique_sequence)[0]\n",
    "                sequence_ids = class_ids[sequence_ids]\n",
    "                sequence_frame_ids = self.duplicate_ids[sequence_ids]\n",
    "\n",
    "                # order chronologically according to frame id\n",
    "                sequence_ids = sequence_ids[np.argsort(sequence_frame_ids)]\n",
    "\n",
    "                ordered_ids += sequence_ids.tolist()\n",
    "\n",
    "        ordered_ids = np.array(ordered_ids)\n",
    "\n",
    "        self.segments = [self.segments[i] for i in ordered_ids]\n",
    "        self.classes = self.classes[ordered_ids]\n",
    "\n",
    "        if self.positions.size > 0:\n",
    "            self.positions = self.positions[ordered_ids]\n",
    "        if self.features.size > 0:\n",
    "            self.features = self.features[ordered_ids]\n",
    "\n",
    "        self.duplicate_ids = self.duplicate_ids[ordered_ids]\n",
    "        self.duplicate_classes = self.duplicate_classes[ordered_ids]\n",
    "\n",
    "    # keep only segments and corresponding data where the keep parameter is true\n",
    "    def _trim_data(self, keep):\n",
    "        self.segments = [segment for (k, segment) in zip(keep, self.segments) if k]\n",
    "        self.classes = self.classes[keep]\n",
    "\n",
    "        if self.positions.size > 0:\n",
    "            self.positions = self.positions[keep]\n",
    "        if self.features.size > 0:\n",
    "            self.features = self.features[keep]\n",
    "\n",
    "        self.duplicate_ids = self.duplicate_ids[keep]\n",
    "        self.duplicate_classes = self.duplicate_classes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes))\n",
    "    for i, cls in enumerate(y):\n",
    "        y_onehot[i, cls] = 1\n",
    "\n",
    "    return y_onehot\n",
    "class Generator(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor,\n",
    "        segment_ids,\n",
    "        n_classes,\n",
    "        train=True,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "    ):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.segment_ids = segment_ids\n",
    "        self.n_classes = n_classes\n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.n_segments = len(self.segment_ids)\n",
    "        self.n_batches = int(np.ceil(float(self.n_segments) / batch_size))\n",
    "\n",
    "        self._i = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        if self.shuffle and self._i == 0:\n",
    "            np.random.shuffle(self.segment_ids)\n",
    "\n",
    "        self.batch_ids = self.segment_ids[self._i : self._i + self.batch_size]\n",
    "#         print(self.segment_ids)\n",
    "        self._i = self._i + self.batch_size\n",
    "        if self._i >= self.n_segments:\n",
    "            self._i = 0\n",
    "\n",
    "        batch_segments, batch_classes = self.preprocessor.get_processed(\n",
    "            self.batch_ids, train=self.train\n",
    "        )\n",
    "\n",
    "        batch_segments = batch_segments[:, :, :, :, None]\n",
    "        batch_classes = to_onehot(batch_classes, self.n_classes)\n",
    "\n",
    "        return batch_segments, batch_classes\n",
    "\n",
    "\n",
    "class GeneratorFeatures(object):\n",
    "    def __init__(self, features, classes, n_classes=2, batch_size=16, shuffle=True):\n",
    "        self.features = features\n",
    "        self.classes = np.asarray(classes)\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_samples = features.shape[0]\n",
    "        self.n_batches = int(np.ceil(float(self.n_samples) / batch_size))\n",
    "        self._i = 0\n",
    "\n",
    "        self.sample_ids = list(range(self.n_samples))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.sample_ids)\n",
    "\n",
    "    def next(self):\n",
    "        batch_ids = self.sample_ids[self._i : self._i + self.batch_size]\n",
    "\n",
    "        self._i = self._i + self.batch_size\n",
    "        if self._i >= self.n_samples:\n",
    "            self._i = 0\n",
    "\n",
    "        batch_features = self.features[batch_ids, :]\n",
    "        batch_classes = self.classes[batch_ids]\n",
    "        batch_classes = to_onehot(batch_classes, self.n_classes)\n",
    "\n",
    "        return batch_features, batch_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segments(folder=folder, filename=\"segments_database.csv\"):\n",
    "    # container\n",
    "    segments = []\n",
    "\n",
    "    # extract and store point data\n",
    "    from pandas import read_csv\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    extracted_data = read_csv(file_path, delimiter=\" \").values\n",
    "\n",
    "    segment_ids = extracted_data[:, 0].astype(int)\n",
    "    duplicate_ids = extracted_data[:, 1].astype(int)\n",
    "    points = extracted_data[:, 2:]\n",
    "\n",
    "#     complete_ids = list(zip(segment_ids, duplicate_ids))\n",
    "    id_changes = []\n",
    "    for i in range(len(segment_ids)):\n",
    "        if i > 0 and (segment_ids[i] != (segment_ids[i - 1]) or duplicate_ids[i]!=duplicate_ids[i-1]):\n",
    "            id_changes.append(i)\n",
    "#     complete_ids = list(zip(segment_ids, duplicate_ids))\n",
    "#     id_changes = []\n",
    "#     for i, complete_id in enumerate(complete_ids):\n",
    "#         if i > 0 and complete_id != complete_ids[i - 1]:\n",
    "#             id_changes.append(i)\n",
    "            \n",
    "    segments = np.split(points, id_changes)\n",
    "\n",
    "    segment_ids = [ids[0] for ids in np.split(segment_ids, id_changes)]\n",
    "    duplicate_ids = [ids[0] for ids in np.split(duplicate_ids, id_changes)]\n",
    "\n",
    "    if len(set(zip(segment_ids, duplicate_ids))) != len(segment_ids):\n",
    "        raise ValueError(\n",
    "            \"Id collision when importing segments. Two segments with same id exist in file.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"  Found \"\n",
    "        + str(len(segments))\n",
    "        + \" segments from \"\n",
    "        + str(np.unique(segment_ids).size)\n",
    "        + \" sequences\"\n",
    "    )\n",
    "    return segments, segment_ids, duplicate_ids\n",
    "\n",
    "def load_segments_no_duplicates(folder=folder, filename=\"segments_database.csv\"):\n",
    "    # container\n",
    "    segments = []\n",
    "\n",
    "    # extract and store point data\n",
    "    from pandas import read_csv\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    extracted_data = read_csv(file_path, delimiter=\" \").values\n",
    "\n",
    "    segment_ids = extracted_data[:, 0].astype(int)\n",
    "    points = extracted_data[:, 1:]\n",
    "\n",
    "    id_changes = []\n",
    "    for i, segment_id in enumerate(segment_ids):\n",
    "        if i > 0 and segment_id != segment_ids[i - 1]:\n",
    "            id_changes.append(i)\n",
    "\n",
    "    segments = np.split(points, id_changes)\n",
    "\n",
    "    segment_ids = [ids[0] for ids in np.split(segment_ids, id_changes)]\n",
    "\n",
    "    print(\"  Found \" + str(len(segments)) + \" segments.\")\n",
    "    return segments, segment_ids\n",
    "\n",
    "\n",
    "def load_positions(folder=folder, filename=\"positions_database.csv\"):\n",
    "    segment_ids = []\n",
    "    duplicate_ids = []\n",
    "    positions = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                segment_ids.append(int(split_line[0]))\n",
    "                duplicate_ids.append(int(split_line[1]))\n",
    "\n",
    "                segment_position = list(map(float, split_line[2:]))\n",
    "                positions.append(segment_position)\n",
    "\n",
    "    print(\"  Found positions for \" + str(len(positions)) + \" segments\")\n",
    "    return positions, segment_ids, duplicate_ids\n",
    "\n",
    "\n",
    "def load_labels(folder=folder, filename=\"labels_database.csv\"):\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                segment_ids.append(int(split_line[0]))\n",
    "                labels.append(int(split_line[1]))\n",
    "\n",
    "    print(\"  Found labels for \" + str(len(labels)) + \" segment ids\")\n",
    "    return labels, segment_ids\n",
    "\n",
    "\n",
    "def load_features(folder=folder, filename=\"features_database.csv\"):\n",
    "    # containers\n",
    "    segment_ids = []\n",
    "    duplicate_ids = []\n",
    "    features = []\n",
    "    feature_names = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if filename:\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "\n",
    "                # feature names\n",
    "                if len(feature_names) == 0:\n",
    "                    feature_names = split_line[2::2]\n",
    "\n",
    "                # id\n",
    "                segment_id = split_line[0]\n",
    "                segment_ids.append(int(segment_id))\n",
    "                duplicate_id = split_line[1]\n",
    "                duplicate_ids.append(int(duplicate_id))\n",
    "\n",
    "                # feature values\n",
    "                features.append(np.array([float(i) for i in split_line[3::2]]))\n",
    "\n",
    "    print(\"  Found features for \" + str(len(features)) + \" segments\", end=\"\")\n",
    "    if \"autoencoder_feature1\" in feature_names:\n",
    "        print(\"(incl. autoencoder features)\", end=\"\")\n",
    "    print(\" \")\n",
    "    return features, feature_names, segment_ids, duplicate_ids\n",
    "\n",
    "def load_matches(folder=folder, filename=\"matches_database.csv\"):\n",
    "    # containers\n",
    "    matches = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as inputfile:\n",
    "            for line in inputfile:\n",
    "                split_line = line.strip().split(\" \")\n",
    "                matches.append([int(float(i)) for i in split_line if i != \"\"])\n",
    "\n",
    "    print(\"  Found \" + str(len(matches)) + \" matches\")\n",
    "    return np.array(matches)\n",
    "\n",
    "\n",
    "def load_merges(folder=folder, filename=\"merge_events_database.csv\"):\n",
    "    merge_timestamps = []\n",
    "    merges = []\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path) as inputfile:\n",
    "        for line in inputfile:\n",
    "            split_line = line.strip().split(\" \")\n",
    "            merge_timestamps.append(int(split_line[0]))\n",
    "            merges.append(list(map(int, split_line[1:])))\n",
    "\n",
    "    print(\"  Found \" + str(len(merges)) + \" merge events\")\n",
    "    return merges, merge_timestamps\n",
    "\n",
    "def load_list_of_lists(path):\n",
    "    with open(path) as infile:\n",
    "        list_of_lists = [line.strip().split(\" \") for line in infile]\n",
    "    return convert_strings_to_floats_in_list_of_lists(list_of_lists)\n",
    "\n",
    "\n",
    "def convert_strings_to_floats_in_list_of_lists(list_of_lists):\n",
    "    result = []\n",
    "    for list_ in list_of_lists:\n",
    "        result_line = []\n",
    "        for item in list_:\n",
    "            try:\n",
    "                num = float(item)\n",
    "                if num.is_integer():\n",
    "                    num = int(num)\n",
    "                result_line.append(num)\n",
    "            except:\n",
    "                result_line.append(item)\n",
    "        result.append(result_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preprocessor=Preprocessor(augment_angle=180,\n",
    "        augment_remove_random_min=0.0,\n",
    "        augment_remove_random_max=0.1,\n",
    "        augment_remove_plane_min=0.0,\n",
    "        augment_remove_plane_max=0.5,\n",
    "        augment_jitter=0.0,\n",
    "        align=\"eigen\",\n",
    "        voxelize=True,\n",
    "        scale_method=\"fit\",\n",
    "        center_method=\"mean\",\n",
    "        scale=(8, 8, 4),\n",
    "        voxels=(32, 32, 16),\n",
    "        remove_mean=False,\n",
    "        remove_std=False,\n",
    "        batch_size=32,\n",
    "        scaler_train_passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 28993 segments from 4441 sequences\n",
      "  Found positions for 28993 segments\n",
      "  Found features for 28993 segments \n",
      "  Found labels for 1972 segment ids\n",
      "  Found 453 matches\n",
      "  Found 28993 segments that changed enough\n",
      "  Found 2251 merge events\n",
      "  Found 264 matches that are relevant after merges\n",
      "  Found 20593 segments that are relevant\n",
      "  Found 259 matches that are unique\n",
      "  Found 233 matches after grouping\n",
      "  Found 1746 valid classes with 20397 segments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder18='dataset18'\n",
    "data18=Dataset(folder=folder18)\n",
    "segments, _, ids, n_ids, features, matches, labels_dict=data18.load(preprocessor=preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold = np.ones(ids.shape, dtype=np.int)\n",
    "for c in np.unique(ids):\n",
    "    dup_classes = data18.duplicate_classes[ids == c]\n",
    "    unique_dup_classes = np.unique(dup_classes)\n",
    "\n",
    "    # choose for train the sequence with the largest last segment\n",
    "    dup_sizes = []\n",
    "    for dup_class in unique_dup_classes:\n",
    "        dup_ids = np.where(dup_class == data18.duplicate_classes)[0]\n",
    "        last_id = np.max(dup_ids)\n",
    "        dup_sizes.append(segments[last_id].shape[0])\n",
    "\n",
    "    dup_keep = np.argmax(dup_sizes)\n",
    "\n",
    "    # randomly distribute the others\n",
    "    for i, dup_class in enumerate(unique_dup_classes):\n",
    "        if i != dup_keep:\n",
    "            if np.random.random() < 0:\n",
    "                train_fold[duplicate_classes == dup_class] = 0\n",
    "\n",
    "train_ids = np.where(train_fold == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor.init_segments(segments, ids,train_ids=ids)\n",
    "gen_train = Generator(\n",
    "    preprocessor,\n",
    "    ids,\n",
    "    n_ids,\n",
    "    train=True,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_roc_pairs(\n",
    "    segments,\n",
    "    classes,\n",
    "    duplicate_classes,\n",
    "    USE_LAST_SAMPLE_ONLY=False,\n",
    "    ALWAYS_AGAINST_LAST=False,\n",
    "    MIN_DISTANCE_NEGATIVES=20.0,\n",
    "):\n",
    "    pair_ids = []\n",
    "    pair_labels = []\n",
    "\n",
    "    # calculate centroids\n",
    "    centroids = [np.mean(segment, 0) for segment in segments]\n",
    "\n",
    "    # positive samples\n",
    "    for cls in np.unique(classes):\n",
    "        class_ids = np.where(classes == cls)[0]\n",
    "\n",
    "        sequences = duplicate_classes[class_ids]\n",
    "        unique_sequences = np.unique(sequences)\n",
    "\n",
    "        if unique_sequences.size > 1:\n",
    "            for i, sequence_1 in enumerate(unique_sequences):\n",
    "                for sequence_2 in unique_sequences[i + 1 :]:\n",
    "                    ids_1 = class_ids[np.where(sequences == sequence_1)[0]]\n",
    "                    ids_2 = class_ids[np.where(sequences == sequence_2)[0]]\n",
    "\n",
    "                    if USE_LAST_SAMPLE_ONLY:\n",
    "                        pair_ids.append(ids_1[-1], ids_2[-1])\n",
    "                        pair_labels.append(1)\n",
    "                    elif ALWAYS_AGAINST_LAST:\n",
    "                        for id_1 in ids_1:\n",
    "                            pair_ids.append([id_1, ids_2[-1]])\n",
    "                            pair_labels.append(1)\n",
    "\n",
    "                        for id_2 in ids_2:\n",
    "                            pair_ids.append([ids_1[-1], id_2])\n",
    "                            pair_labels.append(1)\n",
    "                    else:\n",
    "                        for id_1 in ids_1:\n",
    "                            for id_2 in ids_2:\n",
    "                                pair_ids.append([id_1, id_2])\n",
    "                                pair_labels.append(1)\n",
    "\n",
    "    n_positives = len(pair_ids)\n",
    "\n",
    "    # negative samples\n",
    "    random.seed(54321)\n",
    "    ids = range(len(segments))\n",
    "\n",
    "    last_ids = []\n",
    "    for sequence in np.unique(duplicate_classes):\n",
    "        last_ids.append(np.where(duplicate_classes == sequence)[0][-1])\n",
    "\n",
    "    n_negatives = 0\n",
    "    while n_negatives < n_positives:\n",
    "        id_1 = random.sample(ids, 1)[0]\n",
    "        id_2 = random.sample(ids, 1)[0]\n",
    "\n",
    "        if ALWAYS_AGAINST_LAST:\n",
    "            id_2 = random.sample(last_ids, 1)[0]\n",
    "\n",
    "        dist = np.linalg.norm(centroids[id_1] - centroids[id_2])\n",
    "        if dist >= MIN_DISTANCE_NEGATIVES:\n",
    "            pair_ids.append([id_1, id_2])\n",
    "            pair_labels.append(0)\n",
    "            n_negatives += 1\n",
    "\n",
    "    pair_ids = np.array(pair_ids)\n",
    "    pair_labels = np.array(pair_labels)\n",
    "\n",
    "    print(\"Positive pairs: %d\" % n_positives)\n",
    "    print(\"Negative pairs: %d\" % n_negatives)\n",
    "\n",
    "    return pair_ids, pair_labels\n",
    "\n",
    "\n",
    "def get_roc_curve(features, pair_ids, pair_labels):\n",
    "    y_pred = []\n",
    "    for i in range(pair_ids.shape[0]):\n",
    "        y_pred.append(\n",
    "            1.0\n",
    "            / (\n",
    "                np.linalg.norm(features[pair_ids[i, 0]] - features[pair_ids[i, 1]])\n",
    "                + 1e-12\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(pair_labels, y_pred, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 28993 segments from 4441 sequences\n",
      "  Found positions for 28993 segments\n",
      "  Found features for 28993 segments \n",
      "  Found labels for 1972 segment ids\n",
      "  Found 453 matches\n",
      "  Found 28993 segments that changed enough\n",
      "  Found 2251 merge events\n",
      "  Found 264 matches that are relevant after merges\n",
      "  Found 20593 segments that are relevant\n",
      "  Found 259 matches that are unique\n",
      "  Found 233 matches after grouping\n",
      "  Found 1746 valid classes with 20397 segments\n"
     ]
    }
   ],
   "source": [
    "#!/home/mr/segmappyenv/bin/python3\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "base_dir = 'C:/Users/kichk/Downloads/segmap_machine_learning_project-master/'\n",
    "folder = ''\n",
    "segments = []\n",
    "classes = np.array([], dtype=np.int)\n",
    "n_classes = 0\n",
    "duplicate_classes = np.array([], dtype=np.int)\n",
    "max_duplicate_class = 0\n",
    "duplicate_ids = np.array([], dtype=np.int)\n",
    "\n",
    "runs = cnn_train_folders.split(\",\")\n",
    "for run in runs:\n",
    "    dataset = data18\n",
    "\n",
    "    run_segments, _, run_classes, run_n_classes, _, _, _ = dataset.load(\n",
    "        preprocessor=preprocessor\n",
    "    )\n",
    "    run_duplicate_classes = dataset.duplicate_classes\n",
    "    run_duplicate_ids = dataset.duplicate_ids\n",
    "\n",
    "    run_classes += n_classes\n",
    "    run_duplicate_classes += max_duplicate_class\n",
    "\n",
    "    segments += run_segments\n",
    "    classes = np.concatenate((classes, run_classes), axis=0)\n",
    "    n_classes += run_n_classes\n",
    "    duplicate_classes = np.concatenate(\n",
    "        (duplicate_classes, run_duplicate_classes), axis=0\n",
    "    )\n",
    "    duplicate_ids = np.concatenate((duplicate_ids, run_duplicate_ids), axis=0)\n",
    "\n",
    "    max_duplicate_class = np.max(duplicate_classes) + 1\n",
    "\n",
    "if debug:\n",
    "    import json\n",
    "\n",
    "    # empty or create the debug folder\n",
    "    if os.path.isdir(debug_path):\n",
    "        import glob\n",
    "\n",
    "        debug_files = glob.glob(os.path.join(debug_path, \"*.json\"))\n",
    "        for debug_file in debug_files:\n",
    "            os.remove(debug_file)\n",
    "    else:\n",
    "        os.makedirs(debug_path)\n",
    "\n",
    "    # store loss information\n",
    "    epoch_log = []\n",
    "    train_loss_log = []\n",
    "    train_loss_c_log = []\n",
    "    train_loss_r_log = []\n",
    "    train_accuracy_log = []\n",
    "    test_loss_log = []\n",
    "    test_loss_c_log = []\n",
    "    test_loss_r_log = []\n",
    "    test_accuracy_log = []\n",
    "\n",
    "    # store segment centers for the current run\n",
    "    centers = []\n",
    "    for cls in range(n_classes):\n",
    "        class_ids = np.where(classes == cls)[0]\n",
    "        last_id = class_ids[np.argmax(duplicate_ids[class_ids])]\n",
    "        centers.append(np.mean(segments[last_id], axis=0).tolist())\n",
    "\n",
    "    with open(os.path.join(debug_path, \"centers.json\"), \"w\") as fp:\n",
    "        json.dump(centers, fp)\n",
    "\n",
    "    # info for sequence prediction visualization\n",
    "    pred = [0] * (np.max(duplicate_classes) + 1)\n",
    "    duplicate_ids_norm = np.zeros(duplicate_ids.shape, dtype=np.int)\n",
    "    for duplicate_class in np.unique(duplicate_classes):\n",
    "        segment_ids = np.where(duplicate_classes == duplicate_class)[0]\n",
    "        pred[duplicate_class] = [None] * segment_ids.size\n",
    "\n",
    "        for i, segment_id in enumerate(segment_ids):\n",
    "            duplicate_ids_norm[segment_id] = i\n",
    "\n",
    "    def debug_write_pred(segment_id, segment_probs, train):\n",
    "        top5_classes = np.argsort(segment_probs)[::-1]\n",
    "        top5_classes = top5_classes[:5]\n",
    "        top5_prob = segment_probs[top5_classes]\n",
    "\n",
    "        segment_class = classes[segment_id]\n",
    "        segment_prob = segment_probs[segment_class]\n",
    "\n",
    "        info = [\n",
    "            int(train),\n",
    "            int(segment_class),\n",
    "            float(segment_prob),\n",
    "            top5_classes.tolist(),\n",
    "            top5_prob.tolist(),\n",
    "        ]\n",
    "\n",
    "        duplicate_class = duplicate_classes[segment_id]\n",
    "        duplicate_id = duplicate_ids_norm[segment_id]\n",
    "\n",
    "        pred[duplicate_class][duplicate_id] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 28993 segments from 4441 sequences\n",
      "  Found positions for 28993 segments\n",
      "  Found features for 28993 segments \n",
      "  Found labels for 1972 segment ids\n",
      "  Found 453 matches\n",
      "  Found 28993 segments that changed enough\n",
      "  Found 2251 merge events\n",
      "  Found 264 matches that are relevant after merges\n",
      "  Found 20593 segments that are relevant\n",
      "  Found 259 matches that are unique\n",
      "  Found 233 matches after grouping\n",
      "  Found 1746 valid classes with 20397 segments\n",
      "Positive pairs: 50567\n",
      "Negative pairs: 50567\n"
     ]
    }
   ],
   "source": [
    "# load dataset for calculating roc\n",
    "if roc:\n",
    "    # get test dataset\n",
    "    roc_preprocessor = preprocessor #TODO: check\n",
    "\n",
    "    roc_dataset = data18\n",
    "\n",
    "    roc_segments, roc_positions, roc_classes, roc_n_classes, roc_features, _, _ = roc_dataset.load(\n",
    "        preprocessor=roc_preprocessor\n",
    "    )\n",
    "\n",
    "    roc_duplicate_classes = roc_dataset.duplicate_classes\n",
    "\n",
    "    # get roc positive and negative pairs\n",
    "    pair_ids, pair_labels = get_roc_pairs(\n",
    "        roc_segments, roc_classes, roc_duplicate_classes\n",
    "    )\n",
    "\n",
    "    roc_ids = np.unique(pair_ids)\n",
    "    roc_segments = [roc_segments[roc_id] for roc_id in roc_ids]\n",
    "    roc_classes = roc_classes[roc_ids]\n",
    "    roc_positions = roc_positions[roc_ids]\n",
    "\n",
    "    for i, roc_id in enumerate(roc_ids):\n",
    "        pair_ids[pair_ids == roc_id] = i\n",
    "\n",
    "    roc_preprocessor.init_segments(\n",
    "        roc_segments, roc_classes, positions=roc_positions\n",
    "    )\n",
    "    gen_roc = Generator(\n",
    "        roc_preprocessor,\n",
    "        range(len(roc_segments)),\n",
    "        roc_n_classes,\n",
    "        train=False,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the cnn model\n",
    "def init_model(input_shape, n_classes):\n",
    "    with tf.name_scope(\"InputScope\") as scope:\n",
    "        cnn_input = tf.placeholder(\n",
    "            dtype=tf.float32, shape=(None,) + input_shape + (1,), name=\"input\"\n",
    "        )\n",
    "\n",
    "    # base convolutional layers\n",
    "    y_true = tf.placeholder(dtype=tf.float32, shape=(None, n_classes), name=\"y_true\")\n",
    "\n",
    "    scales = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\"scales\")\n",
    "\n",
    "    training = tf.placeholder_with_default(\n",
    "        tf.constant(False, dtype=tf.bool), shape=(), name=\"training\"\n",
    "    )\n",
    "\n",
    "    conv1 = tf.layers.conv3d(\n",
    "        inputs=cnn_input,\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"conv1\",\n",
    "    )\n",
    "\n",
    "    pool1 = tf.layers.max_pooling3d(\n",
    "        inputs=conv1, pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool1\"\n",
    "    )\n",
    "\n",
    "    conv2 = tf.layers.conv3d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"conv3\",\n",
    "    )\n",
    "\n",
    "    pool2 = tf.layers.max_pooling3d(\n",
    "        inputs=conv2, pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\"\n",
    "    )\n",
    "\n",
    "    conv3 = tf.layers.conv3d(\n",
    "        inputs=pool2,\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"conv5\",\n",
    "    )\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(inputs=conv3)\n",
    "    flatten = tf.concat([flatten, scales], axis=1, name=\"flatten\")\n",
    "\n",
    "    # classification network\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=flatten,\n",
    "        units=512,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        use_bias=True,\n",
    "        name=\"dense1\",\n",
    "    )\n",
    "\n",
    "    bn_dense1 = tf.layers.batch_normalization(\n",
    "        dense1, training=training, name=\"bn_dense1\"\n",
    "    )\n",
    "\n",
    "    dropout_dense1 = tf.layers.dropout(\n",
    "        bn_dense1, rate=0.5, training=training, name=\"dropout_dense1\"\n",
    "    )\n",
    "\n",
    "    descriptor = tf.layers.dense(\n",
    "        inputs=dropout_dense1,\n",
    "        units=64,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        name=\"descriptor\",\n",
    "    )\n",
    "\n",
    "    bn_descriptor = tf.layers.batch_normalization(\n",
    "        descriptor, training=training, name=\"bn_descriptor\"\n",
    "    )\n",
    "\n",
    "    with tf.name_scope(\"OutputScope\") as scope:\n",
    "        tf.add(bn_descriptor, 0, name=\"descriptor_bn_read\")\n",
    "        tf.add(descriptor, 0, name=\"descriptor_read\")\n",
    "\n",
    "    dropout_descriptor = tf.layers.dropout(\n",
    "        bn_descriptor, rate=0.35, training=training, name=\"dropout_descriptor\"\n",
    "    )\n",
    "\n",
    "    y_pred = tf.layers.dense(\n",
    "        inputs=dropout_descriptor,\n",
    "        units=n_classes,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        name=\"classes\",\n",
    "    )\n",
    "\n",
    "    loss_c = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y_true),\n",
    "        name=\"loss_c\",\n",
    "    )\n",
    "\n",
    "    # reconstruction network\n",
    "    dec_dense1 = tf.layers.dense(\n",
    "        inputs=descriptor,\n",
    "        units=8192,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        name=\"dec_dense1\",\n",
    "    )\n",
    "\n",
    "    reshape = tf.reshape(dec_dense1, (tf.shape(cnn_input)[0], 8, 8, 4, 32))\n",
    "\n",
    "    dec_conv1 = tf.layers.conv3d_transpose(\n",
    "        inputs=reshape,\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        strides=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"dec_conv1\",\n",
    "    )\n",
    "\n",
    "    dec_conv2 = tf.layers.conv3d_transpose(\n",
    "        inputs=dec_conv1,\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        strides=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"dec_conv2\",\n",
    "    )\n",
    "\n",
    "    dec_reshape = tf.layers.conv3d_transpose(\n",
    "        inputs=dec_conv2,\n",
    "        filters=1,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        activation=tf.nn.sigmoid,\n",
    "        name=\"dec_reshape\",\n",
    "    )\n",
    "\n",
    "    reconstruction = dec_reshape\n",
    "    with tf.name_scope(\"ReconstructionScopeAE\") as scope:\n",
    "        tf.add(reconstruction, 0, name=\"ae_reconstruction_read\")\n",
    "\n",
    "    FN_TO_FP_WEIGHT = 0.9\n",
    "    loss_r = -tf.reduce_mean(\n",
    "        FN_TO_FP_WEIGHT * cnn_input * tf.log(reconstruction + 1e-10)\n",
    "        + (1 - FN_TO_FP_WEIGHT) * (1 - cnn_input) * tf.log(1 - reconstruction + 1e-10)\n",
    "    )\n",
    "    tf.identity(loss_r, \"loss_r\")\n",
    "\n",
    "    # training\n",
    "    LOSS_R_WEIGHT = 200\n",
    "    LOSS_C_WEIGHT = 1\n",
    "    loss = tf.add(LOSS_C_WEIGHT * loss_c, LOSS_R_WEIGHT * loss_r, name=\"loss\")\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    update_step = tf.assign(\n",
    "        global_step, tf.add(global_step, tf.constant(1)), name=\"update_step\"\n",
    "    )\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "\n",
    "    # add batch normalization updates to the training operation\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss, name=\"train_op\")\n",
    "\n",
    "    # statistics\n",
    "    y_prob = tf.nn.softmax(y_pred, name=\"y_prob\")\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "\n",
    "    roc_auc = tf.placeholder(dtype=tf.float32, shape=(), name=\"roc_auc\")\n",
    "\n",
    "    with tf.name_scope(\"summary\"):\n",
    "        tf.summary.scalar(\"loss\", loss, collections=[\"summary_batch\"])\n",
    "        tf.summary.scalar(\"loss_c\", loss_c, collections=[\"summary_batch\"])\n",
    "        tf.summary.scalar(\"loss_r\", loss_r, collections=[\"summary_batch\"])\n",
    "        tf.summary.scalar(\"accuracy\", accuracy, collections=[\"summary_batch\"])\n",
    "\n",
    "        tf.summary.scalar(\"roc_auc\", roc_auc, collections=[\"summary_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 19605 segments\n",
      "Testing with 792 segments\n",
      "epoch  0 v_loss 25.5272 v_acc 0.00 v_c 7.4604 v_r 0.0903"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-cb6a18a8bfae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    147\u001b[0m                         \u001b[0my_true\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                         \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                         \u001b[0mscales\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_scales\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m                     },\n\u001b[0;32m    151\u001b[0m                 )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m     \u001b[0mr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[0mr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mcallable\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mFor\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m     \u001b[0mthe\u001b[0m \u001b[0mcallable\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[0mnode_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m         \u001b[0mnode_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m           \u001b[0mnode_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1319\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m                           run_metadata):\n\u001b[0;32m   1406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0m\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split so that the test set contains entire sequences\n",
    "prev_epoch = -1\n",
    "train_fold = np.ones(classes.shape, dtype=np.int)\n",
    "for c in np.unique(classes):\n",
    "    dup_classes = duplicate_classes[classes == c]\n",
    "    unique_dup_classes = np.unique(dup_classes)\n",
    "\n",
    "    # choose for train the sequence with the largest last segment\n",
    "    dup_sizes = []\n",
    "    for dup_class in unique_dup_classes:\n",
    "        dup_ids = np.where(dup_class == duplicate_classes)[0]\n",
    "        last_id = np.max(dup_ids)\n",
    "        dup_sizes.append(segments[last_id].shape[0])\n",
    "\n",
    "    dup_keep = np.argmax(dup_sizes)\n",
    "\n",
    "    # randomly distribute the others\n",
    "    for i, dup_class in enumerate(unique_dup_classes):\n",
    "        if i != dup_keep:\n",
    "            if np.random.random() < test_size:\n",
    "                train_fold[duplicate_classes == dup_class] = 0\n",
    "\n",
    "train_ids = np.where(train_fold == 1)[0]\n",
    "test_ids = np.where(train_fold == 0)[0]\n",
    "\n",
    "# initialize preprocessor\n",
    "preprocessor.init_segments(segments, classes, train_ids=train_ids)\n",
    "\n",
    "# save scaler mean in a csv\n",
    "if remove_mean:\n",
    "    scaler_path = os.path.join(cnn_model_folder, \"scaler_mean.csv\")\n",
    "    with open(scaler_path, \"w\") as fp:\n",
    "        for i in preprocessor._scaler.mean_:\n",
    "            fp.write(str(i) + \"\\n\")\n",
    "\n",
    "# save the scaler as well using pickle\n",
    "if remove_mean or remove_std:\n",
    "    scaler_path = os.path.join(cnn_model_folder, \"scaler.pkl\")\n",
    "    preprocessor.save_scaler(scaler_path)\n",
    "\n",
    "# initialize segment batch generators\n",
    "gen_train = Generator(\n",
    "    preprocessor,\n",
    "    train_ids,\n",
    "    n_classes,\n",
    "    train=True,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "gen_test = Generator(\n",
    "    preprocessor,\n",
    "    test_ids,\n",
    "    n_classes,\n",
    "    train=False,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(\"Training with %d segments\" % gen_train.n_segments)\n",
    "print(\"Testing with %d segments\" % gen_test.n_segments)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "retrain = False\n",
    "if retrain:\n",
    "    # restore variable names from previous session\n",
    "    saver = tf.train.import_meta_graph(retrain + \".meta\")\n",
    "else:\n",
    "    # define a new model\n",
    "    init_model(tuple(preprocessor.voxels), n_classes)\n",
    "\n",
    "    # model saver\n",
    "    saver = tf.train.Saver(max_to_keep=checkpoints)\n",
    "\n",
    "# get key tensorflow variables\n",
    "cnn_graph = tf.get_default_graph()\n",
    "\n",
    "cnn_input = cnn_graph.get_tensor_by_name(\"InputScope/input:0\")\n",
    "y_true = cnn_graph.get_tensor_by_name(\"y_true:0\")\n",
    "training = cnn_graph.get_tensor_by_name(\"training:0\")\n",
    "scales = cnn_graph.get_tensor_by_name(\"scales:0\")\n",
    "\n",
    "loss = cnn_graph.get_tensor_by_name(\"loss:0\")\n",
    "loss_c = cnn_graph.get_tensor_by_name(\"loss_c:0\")\n",
    "loss_r = cnn_graph.get_tensor_by_name(\"loss_r:0\")\n",
    "\n",
    "accuracy = cnn_graph.get_tensor_by_name(\"accuracy:0\")\n",
    "y_prob = cnn_graph.get_tensor_by_name(\"y_prob:0\")\n",
    "descriptor = cnn_graph.get_tensor_by_name(\"OutputScope/descriptor_read:0\")\n",
    "roc_auc = cnn_graph.get_tensor_by_name(\"roc_auc:0\")\n",
    "\n",
    "global_step = cnn_graph.get_tensor_by_name(\"global_step:0\")\n",
    "update_step = cnn_graph.get_tensor_by_name(\"update_step:0\")\n",
    "train_op = cnn_graph.get_operation_by_name(\"train_op\")\n",
    "\n",
    "summary_batch = tf.summary.merge_all(\"summary_batch\")\n",
    "summary_epoch = tf.summary.merge_all(\"summary_epoch\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard statistics\n",
    "    if log_name:\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            os.path.join(log_path, log_name, \"train\"), sess.graph\n",
    "        )\n",
    "        test_writer = tf.summary.FileWriter(\n",
    "            os.path.join(log_path, log_name, \"test\")\n",
    "        )\n",
    "\n",
    "    # initialize all tf variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if retrain:\n",
    "        saver.restore(sess,retrain)\n",
    "\n",
    "    # remember best epoch accuracy\n",
    "    if keep_best:\n",
    "        best_accuracy = 0\n",
    "\n",
    "    # sequence of train and test batches\n",
    "    batches = np.array([1] * gen_train.n_batches + [0] * gen_test.n_batches)\n",
    "    for epoch in range(0, n_epochs):\n",
    "        train_loss = 0\n",
    "        train_loss_c = 0\n",
    "        train_loss_r = 0\n",
    "        train_accuracy = 0\n",
    "        train_step = 0\n",
    "\n",
    "        test_loss = 0\n",
    "        test_loss_c = 0\n",
    "        test_loss_r = 0\n",
    "        test_accuracy = 0\n",
    "        test_step = 0\n",
    "\n",
    "        np.random.shuffle(batches)\n",
    "\n",
    "        console_output_size = 0\n",
    "        for step, train in enumerate(batches):\n",
    "            if train:\n",
    "                batch_segments, batch_classes = gen_train.next()\n",
    "\n",
    "                # run optimizer and calculate loss and accuracy\n",
    "                summary, batch_loss, batch_loss_c, batch_loss_r, batch_accuracy, batch_prob, _ = sess.run(\n",
    "                    [summary_batch, loss, loss_c, loss_r, accuracy, y_prob, train_op],\n",
    "                    feed_dict={\n",
    "                        cnn_input: batch_segments,\n",
    "                        y_true: batch_classes,\n",
    "                        training: True,\n",
    "                        scales: preprocessor.last_scales,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                if debug:\n",
    "                    for i, segment_id in enumerate(gen_train.batch_ids):\n",
    "                        debug_write_pred(segment_id, batch_prob[i], train)\n",
    "\n",
    "                if log_name:\n",
    "                    train_writer.add_summary(summary, sess.run(global_step))\n",
    "\n",
    "                train_loss += batch_loss\n",
    "                train_loss_c += batch_loss_c\n",
    "                train_loss_r += batch_loss_r\n",
    "                train_accuracy += batch_accuracy\n",
    "                train_step += 1\n",
    "            else:\n",
    "                batch_segments, batch_classes = gen_test.next()\n",
    "\n",
    "                # calculate test loss and accuracy\n",
    "                summary, batch_loss, batch_loss_c, batch_loss_r, batch_accuracy, batch_prob = sess.run(\n",
    "                    [summary_batch, loss, loss_c, loss_r, accuracy, y_prob],\n",
    "                    feed_dict={\n",
    "                        cnn_input: batch_segments,\n",
    "                        y_true: batch_classes,\n",
    "                        scales: preprocessor.last_scales,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                if debug:\n",
    "                    for i, segment_id in enumerate(gen_test.batch_ids):\n",
    "                        debug_write_pred(segment_id, batch_prob[i], train)\n",
    "\n",
    "                if log_name:\n",
    "                    test_writer.add_summary(summary, sess.run(global_step))\n",
    "\n",
    "                test_loss += batch_loss\n",
    "                test_loss_c += batch_loss_c\n",
    "                test_loss_r += batch_loss_r\n",
    "                test_accuracy += batch_accuracy\n",
    "                test_step += 1\n",
    "\n",
    "            # update training step\n",
    "            sess.run(update_step)\n",
    "\n",
    "            # print results\n",
    "            if (prev_epoch!=epoch):\n",
    "                prev_epoch = epoch\n",
    "                sys.stdout.write(\"\\b\" * console_output_size)\n",
    "\n",
    "                console_output = \"epoch %2d \" % epoch\n",
    "\n",
    "                if train_step:\n",
    "                    console_output += \"loss %.4f acc %.2f c %.4f r %.4f \" % (\n",
    "                        train_loss / train_step,\n",
    "                        train_accuracy / train_step * 100,\n",
    "                        train_loss_c / train_step,\n",
    "                        train_loss_r / train_step,\n",
    "                    )\n",
    "\n",
    "                if test_step:\n",
    "                    console_output += \"v_loss %.4f v_acc %.2f v_c %.4f v_r %.4f\" % (\n",
    "                        test_loss / test_step,\n",
    "                        test_accuracy / test_step * 100,\n",
    "                        test_loss_c / test_step,\n",
    "                        test_loss_r / test_step,\n",
    "                    )\n",
    "\n",
    "                console_output_size = len(console_output)\n",
    "\n",
    "                sys.stdout.write(console_output)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # dump prediction values and loss\n",
    "        if debug:\n",
    "            epoch_debug_file = os.path.join(debug_path, \"%d.json\" % epoch)\n",
    "            with open(epoch_debug_file, \"w\") as fp:\n",
    "                json.dump(pred, fp)\n",
    "\n",
    "            epoch_log.append(epoch)\n",
    "            train_loss_log.append(train_loss / train_step)\n",
    "            train_loss_c_log.append(train_loss_c / train_step)\n",
    "            train_loss_r_log.append(train_loss_r / train_step)\n",
    "            train_accuracy_log.append(train_accuracy / train_step * 100)\n",
    "            test_loss_log.append(test_loss / test_step)\n",
    "            test_loss_c_log.append(test_loss_c / test_step)\n",
    "            test_loss_r_log.append(test_loss_r / test_step)\n",
    "            test_accuracy_log.append(test_accuracy / test_step * 100)\n",
    "\n",
    "            with open(os.path.join(debug_path, \"loss.json\"), \"w\") as fp:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"epoch\": epoch_log,\n",
    "                        \"train_loss\": train_loss_log,\n",
    "                        \"train_loss_c\": train_loss_c_log,\n",
    "                        \"train_loss_r\": train_loss_r_log,\n",
    "                        \"train_accuracy\": train_accuracy_log,\n",
    "                        \"test_loss\": test_loss_log,\n",
    "                        \"test_loss_c\": test_loss_c_log,\n",
    "                        \"test_loss_r\": test_loss_r_log,\n",
    "                        \"test_accuracy\": test_accuracy_log,\n",
    "                    },\n",
    "                    fp,\n",
    "                )\n",
    "\n",
    "        # calculate roc\n",
    "        if roc:\n",
    "            cnn_features = []\n",
    "            for batch in range(gen_roc.n_batches):\n",
    "                batch_segments, _ = gen_roc.next()\n",
    "\n",
    "                batch_descriptors = sess.run(\n",
    "                    descriptor,\n",
    "                    feed_dict={\n",
    "                        cnn_input: batch_segments,\n",
    "                        scales: roc_preprocessor.last_scales,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                for batch_descriptor in batch_descriptors:\n",
    "                    cnn_features.append(batch_descriptor)\n",
    "\n",
    "            cnn_features = np.array(cnn_features)\n",
    "\n",
    "            _, _, epoch_roc_auc = get_roc_curve(cnn_features, pair_ids, pair_labels)\n",
    "\n",
    "            summary = sess.run(summary_epoch, feed_dict={roc_auc: epoch_roc_auc})\n",
    "            test_writer.add_summary(summary, sess.run(global_step))\n",
    "\n",
    "            sys.stdout.write(\" roc: %.3f\" % epoch_roc_auc)\n",
    "\n",
    "        # flush tensorboard log\n",
    "        if log_name:\n",
    "            train_writer.flush()\n",
    "            test_writer.flush()\n",
    "\n",
    "        # save epoch model\n",
    "        if not keep_best or test_accuracy > best_accuracy:\n",
    "            if checkpoints > 1:\n",
    "                model_name = \"model-%d.ckpt\" % sess.run(global_step)\n",
    "            else:\n",
    "                model_name = \"model.ckpt\"\n",
    "\n",
    "            saver.save(sess, os.path.join(cnn_model_folder, model_name))\n",
    "            tf.train.write_graph(\n",
    "                sess.graph.as_graph_def(), cnn_model_folder, \"graph.pb\"\n",
    "            )\n",
    "\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
